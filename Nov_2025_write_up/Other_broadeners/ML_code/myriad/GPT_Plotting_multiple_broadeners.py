# diagnostics_plot_other_broadeners.py
from __future__ import annotations
from pathlib import Path
import re

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# ---- Config ----
# 1) Set OUT_DIR explicitly, or leave None to auto-pick latest "other_broadeners_*" under ~/Desktop/line_broadening.nosync/Scratch
OUT_DIR: str | None = None
MYRIAD = False

LOCAL = True
if LOCAL:
    BASE = Path.home() / "Desktop" / "line_broadening.nosync" / "Scratch"
else:
    BASE = Path.home() / "Scratch"


# ---- Helpers ----
def pick_latest_run(base: Path) -> Path:
    if MYRIAD:
        candidates = sorted(
            [p for p in base.glob("myriad_other_broadeners_*") if p.is_dir()],
            key=lambda p: p.stat().st_mtime,
            reverse=True,
        )
    else:
        candidates = sorted(
            [p for p in base.glob("other_broadeners_*") if p.is_dir()],
            key=lambda p: p.stat().st_mtime,
            reverse=True,
        )
    if not candidates:
        raise FileNotFoundError(f"No run dirs found under {base}")
    return candidates[0]


def safe_read_csv(p: Path) -> pd.DataFrame:
    if not p.exists():
        return pd.DataFrame()
    return pd.read_csv(p)


def ensure_dir(p: Path) -> None:
    p.mkdir(parents=True, exist_ok=True)


def nice_name(s: str) -> str:
    # compact long comma-joined fold names for filenames
    return re.sub(r"[^A-Za-z0-9._-]+", "_", s)[:180]


# ---- Locate run ----
out_dir = Path(OUT_DIR) if OUT_DIR else pick_latest_run(BASE)
plots_dir = out_dir / "plots"
ensure_dir(plots_dir)

print(f"Using run: {out_dir}")

# ---- Load summaries ----
cv = safe_read_csv(out_dir / "cv_summary.csv")
features = safe_read_csv(out_dir / "features.csv")

# Optional predictions for test-set diagnostics
pred_files = sorted(out_dir.glob("predictions_*.csv"))


def compute_fold_uncertainty(pred_files: list[Path], n_folds: int) -> np.ndarray:
    """
    Return an array of length n_folds with the mean ABSOLUTE uncertainty per fold.
    Assumes 'y_error' in the CSV is FRACTIONAL error (as generated by the training script).
    """
    mean_unc = np.full(n_folds, np.nan)

    if not pred_files:
        return mean_unc

    for i, f in enumerate(sorted(pred_files)):
        if i >= n_folds:
            break
        pred = safe_read_csv(f)
        if pred.empty:
            continue

        # logic: The training script saved 'fractional_error' into 'y_error'.
        # We must convert back to absolute to compare with MAE.
        if "y_error" in pred.columns and "y" in pred.columns:
            # Absolute Uncertainty = Fractional Error * y
            # Use abs(y) just in case, though gamma should be positive
            abs_unc = pd.to_numeric(pred["y_error"], errors="coerce") * pred["y"].abs()
            mean_unc[i] = abs_unc.mean()

    return mean_unc


# ---- Plot 1: Fold-level metrics (Pair-Averaged RMSE) ----
pred_files = sorted(out_dir.glob("predictions_*.csv"))

if pred_files:
    fold_metrics = []
    fold_labels = []

    for f in pred_files:
        df = safe_read_csv(f)
        if df.empty:
            continue

        # 1. Convert fractional error back to absolute uncertainty
        if "y_error" in df.columns and "y" in df.columns:
            df["abs_unc"] = (pd.to_numeric(df["y_error"], errors="coerce") * df["y"]).abs()
        else:
            df["abs_unc"] = np.nan

        # 2. Calculate SQUARED Error per line
        df["sq_err"] = (df["y"] - df["y_pred"]) ** 2

        # 3. Group by Pair to get Pair-level stats
        pair_stats = df.groupby("pair")[["sq_err", "abs_unc"]].mean()

        # 4. Convert MSE to RMSE per pair
        pair_rmses = np.sqrt(pair_stats["sq_err"])

        # 5. Average across all pairs in this fold (Macro-average)
        fold_rmse = pair_rmses.mean()
        fold_unc = pair_stats["abs_unc"].mean()

        label = f.stem.replace("predictions_", "")
        fold_labels.append(label)

        fold_metrics.append({
            "rmse": fold_rmse,
            "unc": fold_unc
        })

    if fold_metrics:
        metrics_df = pd.DataFrame(fold_metrics)
        n_folds = len(metrics_df)
        x = np.arange(n_folds)

        if any(len(L) > 20 for L in fold_labels):
            display_labels = [f"Fold {i + 1}" for i in range(n_folds)]
        else:
            display_labels = fold_labels

        fig = plt.figure(figsize=(8, 5), dpi=160)
        ax = plt.gca()
        width = 0.35

        # Plot RMSE vs Uncertainty
        ax.bar(x - width / 2, metrics_df["rmse"], width=width, label="Pair-averaged RMSE")
        ax.bar(x + width / 2, metrics_df["unc"], width=width, label="Pair-averaged Mean Unc.")

        ax.set_xticks(x)
        ax.set_xticklabels(display_labels, rotation=45, ha="right")
        ax.set_ylabel("γ  [cm$^{-1}$ atm$^{-1}$]")
        ax.set_title("Fold RMSE vs Uncertainty (Averaged per Pair)")
        ax.legend()

        fig.tight_layout()
        fig.savefig(plots_dir / "fold_rmse_vs_uncertainty_pair_averaged.png")
        plt.close(fig)
        print("Saved pair-averaged fold RMSE plot.")

else:
    print("No predictions_*.csv found; skipping fold metrics plot.")

# ---- Plot 2: Per-pair weighted RMSE by fold ----
per_pair_files = sorted(out_dir.glob("per_pair_*.csv"))
if not per_pair_files:
    print("No per_pair_*.csv found; skipping per-pair charts.")

for f in per_pair_files:
    df = safe_read_csv(f)
    if df.empty:
        continue

    df = df.copy()

    # Calculate wRMSE from wMSE
    if "wMSE" in df.columns:
        df["wRMSE"] = np.sqrt(df["wMSE"])
        metric = "wRMSE"
        xlabel = "Weighted RMSE (cm$^{-1}$ atm$^{-1}$)"
    elif "wMAE" in df.columns:
        metric = "wMAE"
        xlabel = "Weighted MAE (cm$^{-1}$ atm$^{-1}$)"
    else:
        continue

    # Sort by the metric (RMSE) descending
    df = df.sort_values(metric, ascending=False)

    labels = df["pair"].astype(str).str.slice(0, 40)

    fig = plt.figure(figsize=(10, max(3, 0.35 * len(df))), dpi=160)
    ax = plt.gca()
    y = np.arange(len(df))

    vals = df[metric]
    ax.barh(y, vals.values)
    ax.set_yticks(y)
    ax.set_yticklabels(labels)
    ax.invert_yaxis()

    ax.set_xlabel(xlabel)
    ax.set_title(f"Per-pair errors: {f.stem.replace('per_pair_', '')}")

    fig.tight_layout()
    fig.savefig(plots_dir / f"{f.stem[:20]}_{metric}.png")
    plt.close(fig)

# ---- Optional Plot 3: Parity and feature-wise plots ----
pred_files = sorted(out_dir.glob("predictions_*.csv"))
if not pred_files:
    print("No predictions_*.csv found; skipping parity and feature-wise plots.")

for f in pred_files:
    pred = safe_read_csv(f)
    if pred.empty or not {"y", "y_pred"}.issubset(pred.columns):
        continue

    # Parity plot
    fig = plt.figure(figsize=(5, 5), dpi=160)
    ax = plt.gca()
    y = pred["y"].to_numpy(dtype=float)
    yhat = pred["y_pred"].to_numpy(dtype=float)
    lim = [0, max(y.max(), yhat.max()) * 1.05]
    ax.scatter(y, yhat, s=8, alpha=0.6)
    ax.plot(lim, lim, lw=1)
    ax.set_xlim(lim);
    ax.set_ylim(lim)
    ax.set_xlabel("Observed γ  [cm$^{-1}$ atm$^{-1}$]")
    ax.set_ylabel("Predicted γ  [cm$^{-1}$ atm$^{-1}$]")
    ax.set_title(f"Parity: {f.stem.replace('predictions_', '')}")
    fig.tight_layout()
    fig.savefig(plots_dir / f"{f.stem[:20]}_parity.png")
    plt.close(fig)

    # Feature-wise plot vs 'm' or 'M' if present
    xcol = "M" if "M" in pred.columns else None
    if xcol:
        for pair, g in pred.groupby("pair"):
            if g.empty:
                continue

            x = g[xcol].to_numpy(dtype=float)
            y = g["y"].to_numpy(dtype=float)
            yhat = g["y_pred"].to_numpy(dtype=float)

            # --- METRIC CALCULATIONS ---
            rmse = np.sqrt(np.mean((y - yhat) ** 2))

            if "y_error" in g.columns:
                abs_unc = (pd.to_numeric(g["y_error"], errors="coerce") * g["y"]).abs()
                mean_lit_unc = abs_unc.mean()
            else:
                mean_lit_unc = np.nan

            # --- PLOTTING ---
            fig = plt.figure(figsize=(9, 4.5), dpi=160)
            ax = plt.gca()

            ax.plot(x, y, "x", color="tab:red", alpha=0.5, markeredgewidth=1.5, label="Literature")
            ax.plot(x, yhat, ".", color="tab:green", alpha=0.7, label="Predicted")

            x_min, x_max = np.nanmin(x), np.nanmax(x)
            x_span = x_max - x_min if x_max != x_min else 1.0
            pos_rmse = x_min - (0.1 * x_span)
            pos_unc = x_max + (0.1 * x_span)
            y_anchor = np.nanmean(y)

            ax.errorbar(pos_rmse, y_anchor, yerr=rmse, fmt='none',
                        ecolor='tab:green', elinewidth=2.5, capsize=5,
                        label=f"Model RMSE: {rmse:.1g}")

            if np.isfinite(mean_lit_unc):
                ax.errorbar(pos_unc, y_anchor, yerr=mean_lit_unc, fmt='none',
                            ecolor='tab:red', elinewidth=2.5, capsize=5, alpha=0.6,
                            label=f"Mean Lit. Unc.: {mean_lit_unc:.1g}")

            ax.set_xlabel(xcol)
            ax.set_ylabel("γ  [cm$^{-1}$ atm$^{-1}$]")
            ax.set_title(f"{str(pair)[:60]}")
            ax.set_xlim(pos_rmse - (0.05 * x_span), pos_unc + (0.05 * x_span))
            ymax = max(np.nanmax(y), np.nanmax(yhat))
            ax.set_ylim(0, ymax * 1.2 if np.isfinite(ymax) else 1)
            ax.legend(loc='best', fontsize=8, framealpha=0.9)
            fig.tight_layout()
            save_name = nice_name(f"scatter_{pair}")
            fig.savefig(plots_dir / f"{save_name}.png")
            plt.close(fig)


# ---- Run-level comparison across multiple out_dirs ----
def summarise_cv_for_run(run_dir: Path, fallback_unc: pd.Series | None = None) -> dict | None:
    """
    Summarise a single run using *unweighted* metrics computed from predictions_*.csv,
    so results are comparable across runs regardless of training/sample weighting.

    Adds:
      - mean_norm_err = mean( RMSE_pair / abs_unc_pair ) over pairs with abs_unc > 0
      - frac_norm_err_le_1 = fraction of pairs with RMSE_pair <= abs_unc_pair
    """
    cv_path = run_dir / "cv_summary.csv"
    if not cv_path.exists():
        return None

    # --- Collect all prediction files for this run ---
    pred_files_run = sorted(run_dir.glob("predictions_*.csv"))
    if not pred_files_run:
        return None

    dfs = []
    for f in pred_files_run:
        df = safe_read_csv(f)
        if df.empty or "y" not in df.columns or "y_pred" not in df.columns:
            continue

        sub = df[["pair", "y", "y_pred"]].copy()

        if "y_error" in df.columns:
            sub["abs_unc"] = (pd.to_numeric(df["y_error"], errors="coerce") * sub["y"]).abs()
        else:
            sub["abs_unc"] = np.nan

        dfs.append(sub)

    if not dfs:
        return None

    full_df = pd.concat(dfs, ignore_index=True)

    # --- Run-level unweighted metrics (independent of training weights) ---
    y = full_df["y"].astype(float).to_numpy()
    y_pred = full_df["y_pred"].astype(float).to_numpy()
    err = y - y_pred

    mae = float(np.mean(np.abs(err)))
    mse = float(np.mean(err**2))
    rmse = float(np.sqrt(mse))

    # R² from raw y / y_pred
    y_mean = float(np.mean(y))
    ss_tot = float(np.sum((y - y_mean) ** 2))
    ss_res = float(np.sum(err ** 2))
    if ss_tot > 0 and len(y) > 1:
        r2 = 1.0 - ss_res / ss_tot
    else:
        r2 = np.nan

    summary: dict[str, float | str] = {
        "run": run_dir.name,
        "r2_mean": r2,
        "mae_mean": mae,
        "mse_mean": mse,
        "rmse_mean": rmse,
    }

    # --- Pair-averaged statistics ---
    full_df["sq_err"] = err**2
    pair_stats = full_df.groupby("pair")[["sq_err", "abs_unc"]].mean()

    # Fallback uncertainty if requested
    if fallback_unc is not None:
        pair_stats["abs_unc"] = pair_stats["abs_unc"].fillna(fallback_unc)

    pair_rmses = np.sqrt(pair_stats["sq_err"])

    summary["pair_avg_rmse"] = float(pair_rmses.mean())
    summary["pair_avg_unc"] = float(pair_stats["abs_unc"].mean())

    # --- Option A: normalised error = RMSE_pair / abs_unc_pair ---
    valid_norm = pair_stats["abs_unc"] > 0
    if valid_norm.any():
        norm_err = pair_rmses[valid_norm] / pair_stats.loc[valid_norm, "abs_unc"]
        mean_norm_err = float(norm_err.mean())
        frac_norm_err_le_1 = float((norm_err <= 1.0).mean())
    else:
        mean_norm_err = np.nan
        frac_norm_err_le_1 = np.nan

    summary["mean_norm_err"] = mean_norm_err
    summary["frac_norm_err_le_1"] = frac_norm_err_le_1
    # keep old name for backwards compatibility if you want
    summary["prop_within_error"] = frac_norm_err_le_1

    return summary


# locate sibling run directories
run_parent = out_dir.parent
if MYRIAD:
    run_dirs = sorted(
        d for d in run_parent.glob("myriad_other_broadeners_*") if d.is_dir()
    )
else:
    run_dirs = sorted(
        d for d in run_parent.glob("other_broadeners_*") if d.is_dir()
    )


summaries: list[dict] = []
for d in run_dirs:
    # 2. Pass the reference to the summary function
    s = summarise_cv_for_run(d)
    if s is not None:
        if "prop_within_error" in s:
            print(f"{s['run']}: {s['prop_within_error']:.1%} within exp. error")
        summaries.append(s)

if summaries:
    runs_df = pd.DataFrame(summaries).set_index("run")
    runs_df = runs_df.sort_index()  # alphabetical by run name

    if MYRIAD:
        compare_dir = BASE / "Compare_runs_myriad"
    else:
        compare_dir = BASE / "Compare_runs"
    ensure_dir(compare_dir)

    # Save CSV (Including prop_within_error)
    runs_df.to_csv(compare_dir / "run_comparison_summary.csv")
    print(f"Saved comparison summary to {compare_dir / 'run_comparison_summary.csv'}")

    x = np.arange(len(runs_df))
    rmse_mean = np.sqrt(runs_df["mse_mean"])
    clean_labels = [str(s).replace("other_broadeners_", "")[:20] for s in runs_df.index]

    # main comparison: R², MAE, MSE per run
    fig, axes = plt.subplots(4, 1, figsize=(10, 10), sharex=True, dpi=160)

    axes[0].bar(x, runs_df["r2_mean"])
    axes[0].set_ylabel("R² (mean)")

    axes[1].bar(x, runs_df["mae_mean"])
    axes[1].set_ylabel("MAE (mean)")

    axes[2].bar(x, rmse_mean)
    axes[2].set_ylabel("RMSE (mean)")

    # --- New subplot: mean normalised error (RMSE_pair / abs_unc_pair) ---
    if "mean_norm_err" in runs_df.columns:
        axes[3].bar(x, runs_df["mean_norm_err"])
        axes[3].set_ylabel("Mean normalised error")
    else:
        axes[3].bar(x, runs_df["prop_within_error"] * 100.0)
        axes[3].set_ylabel("% within exp. err.")

    axes[3].set_xticks(x)
    axes[3].set_xticklabels(clean_labels, rotation=45, ha="right")

    fig.suptitle("Cross-run CV comparison")
    fig.tight_layout()
    fig.savefig(compare_dir / "run_comparison_metrics.png")
    plt.close(fig)

    # optional: error vs uncertainty per run (Side-by-Side, Pair-Averaged)
    if "pair_avg_rmse" in runs_df.columns and "pair_avg_unc" in runs_df.columns:
        fig = plt.figure(figsize=(10, 5), dpi=160)
        ax = plt.gca()
        width = 0.35

        ax.bar(x - width / 2, runs_df["pair_avg_rmse"], width=width, label="Pair-averaged RMSE")
        ax.bar(x + width / 2, runs_df["pair_avg_unc"], width=width, label="Pair-averaged Mean Unc.")

        ax.set_xticks(x)
        ax.set_xticklabels(clean_labels, rotation=45, ha="right")
        ax.set_ylabel("γ  [cm$^{-1}$ atm$^{-1}$]")
        ax.set_title("Global Run Performance: Model Error vs. Experimental Noise")
        ax.legend()

        fig.tight_layout()
        fig.savefig(compare_dir / "run_comparison_rmse_vs_uncertainty.png")
        plt.close(fig)
else:
    print("No comparable runs found for cross-run comparison.")


def compute_per_pair_fractional_scores(
        pred_files: list[Path],
        pair_col: str = "pair",
        y_col: str = "y",
        y_pred_col: str = "y_pred",
        w_col: str = "w",
        y_err_col: str = "y_error",
) -> pd.DataFrame:
    """
    Compute per-pair ML fractional error...
    """
    dfs = []
    for f in pred_files:
        df = safe_read_csv(f)
        if not df.empty:
            dfs.append(df)

    if not dfs:
        return pd.DataFrame()

    df = pd.concat(dfs, ignore_index=True)
    denom = df[y_col].abs().clip(lower=1e-12)
    df["ml_frac_err"] = (df[y_pred_col] - df[y_col]).abs() / denom

    if y_err_col in df.columns:
        df["meas_frac_err"] = df[y_err_col].abs()
    else:
        df["meas_frac_err"] = np.nan

    if w_col in df.columns:
        df["w_eff"] = df[w_col].astype(float)
    else:
        df["w_eff"] = 1.0

    def wmean(x, w):
        x = x.to_numpy()
        w = w.to_numpy()
        mask = np.isfinite(x) & np.isfinite(w)
        if not mask.any():
            return np.nan
        w = w[mask]
        x = x[mask]
        return float((w * x).sum() / w.sum())

    records = []
    for pair, g in df.groupby(pair_col):
        n = len(g)
        ml_frac_mae = wmean(g["ml_frac_err"], g["w_eff"])
        ml_frac_med = float(g["ml_frac_err"].median())
        meas_frac_med = float(g["meas_frac_err"].median()) if "meas_frac_err" in g else np.nan
        ml_over = ml_frac_mae / meas_frac_med if meas_frac_med > 0 else np.nan

        records.append({
            pair_col: pair,
            "n_lines": n,
            "ml_frac_mae": ml_frac_mae,
            "ml_frac_med": ml_frac_med,
            "meas_frac_med": meas_frac_med,
            "ml_over_meas": ml_over,
        })

    out = pd.DataFrame(records).sort_values("ml_frac_mae")
    return out


per_pair = compute_per_pair_fractional_scores(pred_files)

if not per_pair.empty:
    per_pair_sorted = per_pair.sort_values("ml_frac_mae")
    vals = per_pair_sorted["ml_frac_mae"].to_numpy()
    names = per_pair_sorted["pair"].to_numpy()

    threshold = max(np.percentile(vals, 95), 5 * np.median(vals))
    mask_out = vals > threshold

    fig, ax = plt.subplots(figsize=(12, 4), dpi=160)
    vals_clipped = np.where(mask_out, threshold, vals)
    ax.bar(np.arange(len(vals)), vals_clipped)
    ax.set_ylim(0, threshold)
    ax.set_ylabel("ML fractional MAE")
    ax.set_title("Per-pair ML fractional error (clipped for readability)")

    for i, (is_out, v, nm) in enumerate(zip(mask_out, vals, names)):
        if is_out:
            ax.text(i, threshold, f"{nm}\n{v:.1f}", ha="center", va="bottom", fontsize=7, rotation=90)

    ax.set_xticks([])
    fig.tight_layout()
    fig.savefig(plots_dir / "per_pair_ml_fractional_error_clipped.png")
    plt.close(fig)

    fig, ax = plt.subplots(figsize=(6, 12), dpi=160)
    ax.hlines(y=np.arange(len(vals)), xmin=0, xmax=vals)
    ax.plot(vals, np.arange(len(vals)), "o")
    ax.set_yticks(np.arange(len(names)))
    ax.set_yticklabels(names, fontsize=6)
    ax.set_xlabel("ML fractional MAE")
    ax.set_title("Per-pair ML fractional error")
    fig.tight_layout()
    fig.savefig(plots_dir / "per_pair_ml_fractional_error_lollipop.png")
    plt.close(fig)


def compute_global_fractional_error_summary(pred_files: list[Path], y_col: str = "y", y_pred_col: str = "y_pred",
                                            pair_col: str = "pair") -> pd.DataFrame:
    dfs = []
    for f in pred_files:
        df = safe_read_csv(f)
        if not df.empty:
            dfs.append(df)

    if not dfs:
        return pd.DataFrame()

    df = pd.concat(dfs, ignore_index=True)
    denom = df[y_col].abs().clip(lower=1e-12)
    df["ml_frac_err"] = (df[y_pred_col] - df[y_col]).abs() / denom
    mean_err = df["ml_frac_err"].mean()
    std_err = df["ml_frac_err"].std()
    n = len(df)
    n_pairs = df[pair_col].nunique()

    return pd.DataFrame([{
        "mean_ml_fractional_error": mean_err,
        "std_ml_fractional_error": std_err,
        "n_samples": n,
        "n_pairs": n_pairs,
    }])


summary = compute_global_fractional_error_summary(pred_files)
if not summary.empty:
    out_csv = plots_dir / "global_ml_fractional_error_summary.csv"
    summary.to_csv(out_csv, index=False)
    print(f"Saved global ML fractional error summary to {out_csv}")

print(f"Plots saved to: {plots_dir}")