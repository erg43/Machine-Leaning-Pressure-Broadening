{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-06T15:04:45.301342Z",
     "start_time": "2024-09-06T15:04:45.300454Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "### Set up environment\n",
    "\n",
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import iglob\n",
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "import random\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "#from matplotlib.pyplot import figure\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import glob\n",
    "\n",
    "import json\n",
    "import csv\n",
    "\n",
    "# Set parameters\n",
    "T = 298      # Kelvin\n",
    "\n",
    "\n",
    "# define Jeanna's broadening formalism for use later.  Taken from paper ...\n",
    "def broadening(m, T, ma, mp, b0):\n",
    "    gamma = 1.7796e-5 * (m/(m-2)) * (1/np.sqrt(T)) * np.sqrt((ma+mp)/(ma*mp)) * b0**2\n",
    "    return(gamma)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### Read in data\n",
    "\n",
    "# import data\n",
    "\n",
    "# absolute path to folder containing data\n",
    "#rootdir_glob = '/Users/elizabeth/Desktop/line_broadening.nosync/line_broadening/hitran_data/**/*'\n",
    "# be selective for data files\n",
    "#file_list = [f for f in iglob(rootdir_glob, recursive=True) if os.path.isfile(f) if f[-10:] == \"/1_iso.csv\" if \"readme\" not in f]\n",
    "#file_list = \n",
    "#path = '/Users/elizabeth/Desktop/line_broadening.nosync/line_broadening/model_search/*'\n",
    "#file_list = glob.glob(path + 'raw_data/*.csv')\n",
    "\n",
    "path = '/Users/elizabeth/Desktop/line_broadening.nosync/line_broadening/Other_broadeners/Files_with_new_data/'\n",
    "file_list = glob.glob(path+'*.csv')\n",
    "\n",
    "# read data files, taking the filename from absolute path\n",
    "db = {}\n",
    "for f in file_list:\n",
    "    i = f[101:104].strip('_')\n",
    "    j = f[106:109].strip('_')\n",
    "    file = pd.read_csv(f, low_memory=False)\n",
    "    if i+j in db:\n",
    "        db[i+'_'+j] = pd.concat([db[i+'_'+j], file]).reset_index(drop=True)\n",
    "    else:\n",
    "        db[i+'_'+j] = file.reset_index(drop=True)\n",
    "        \n",
    "print(len(db))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-06T15:04:49.294282Z",
     "start_time": "2024-09-06T15:04:45.946793Z"
    }
   },
   "id": "1d550a34f6b2e775"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "    \n",
    "\n",
    "# dictionary of molecules of data - condensed version\n",
    "molecules = {}\n",
    "\n",
    "# take only molecules for which there is full data\n",
    "for key, datas in db.items():\n",
    "    if key == 'HONO_prediction':\n",
    "        continue\n",
    "    # filter for rotational constant 3D - as that was the last thing added\n",
    "    if 'B0a' in datas.columns:\n",
    "\n",
    "\n",
    "\n",
    "        broadeners = ['air', 'self', 'CO2', 'H2O', 'H2', 'He']\n",
    "        for broadener in broadeners:\n",
    "            print(key, broadener)\n",
    "            if broadener == 'He':\n",
    "                continue\n",
    "            elif broadener == 'air':\n",
    "                continue\n",
    "            elif broadener == 'self':\n",
    "                broadener_data = db[key][['molecule_weight', 'm', 'd',\n",
    "                     'molecule_dipole', 'polar', 'B0a', 'B0b', 'B0c']].loc[0]\n",
    "                \n",
    "            else:\n",
    "                broadener_data = db[broadener][['molecule_weight', 'm', 'd',\n",
    "                     'molecule_dipole', 'polar', 'B0a', 'B0b', 'B0c']].loc[0]\n",
    "                \n",
    "            data = datas[['J', 'Jpp', 'molecule_weight', 'm', 'd', \n",
    "                     'molecule_dipole', 'polar', 'B0a', 'B0b', 'B0c', 'Ka_aprox',\n",
    "                     'Kapp_aprox', 'Kc_aprox', 'Kcpp_aprox', 'gamma_'+broadener, 'gamma_'+broadener+'-err']]\n",
    "            data = data.rename(columns={\"gamma_\"+broadener: \"gamma\", \"gamma_\"+broadener+\"-err\": \"gamma-err\"})\n",
    "            for item in broadener_data.index:\n",
    "                data['broadener_'+item] = broadener_data.loc[item]\n",
    "           \n",
    "\n",
    "            data['m'] = data['m'].loc[0]-4\n",
    "            data['broadener_m'] = data['broadener_m'].loc[0]-4\n",
    "            data['broadener_weight'] = data['broadener_molecule_weight'].loc[0]\n",
    "            data['broadener_dipole'] = data['broadener_molecule_dipole'].loc[0]\n",
    "            data.drop(columns=['broadener_molecule_weight', 'broadener_molecule_dipole'])\n",
    "\n",
    "            # some data missing J values, just get rid of it.\n",
    "            data = data.dropna()\n",
    "        \n",
    "        \n",
    "            #if key == 'C2H6':\n",
    "            #    data = data.drop(data[(data['J'] > 20) & (data['gamma_'+broadener] > 0.08)].index)\n",
    "        \n",
    "        \n",
    "            branch = data['Jpp'] - data['J']\n",
    "            data = data.drop(branch[abs(branch)>2].index)\n",
    "            branch = data['Jpp'] - data['J']\n",
    "        \n",
    "            data['P'] = -data['Jpp'][branch==1]\n",
    "            data['Q'] = data['Jpp'][branch==0]\n",
    "            data['R'] = data['Jpp'][branch==-1] +1\n",
    "            data['O'] = -data['Jpp'][branch==2]\n",
    "            data['S'] = data['Jpp'][branch==-2] +1\n",
    "\n",
    "            data['P'] = data['P'].fillna(0)\n",
    "            data['Q'] = data['Q'].fillna(0)\n",
    "            data['R'] = data['R'].fillna(0)\n",
    "            data['O'] = data['O'].fillna(0)\n",
    "            data['S'] = data['S'].fillna(0)\n",
    "            #data = data.fillna(0)\n",
    "\n",
    "            data['M'] = data['P'] + data['Q'] + data['R'] + data['O'] + data['S']\n",
    "            data = data.drop(columns=['P', 'Q', 'R', 'O', 'S'])\n",
    "            #data['M'] = data['M']/data['B0c']\n",
    "            # assign data to molecule\n",
    "            molecules[key+'_'+broadener] = data\n",
    "\n",
    "\n",
    "             \n",
    "            if key == 'CH3CN':\n",
    "                broadness_jeanna = broadening(data['m'][20]+data['broadener_m'][20], T, data['molecule_weight'][20], data['broadener_weight'][20], data['d'][20]/2+data['broadener_d'][20]/2)\n",
    "            else:\n",
    "                broadness_jeanna = broadening(data['m'][2]+data['broadener_m'][2], T, data['molecule_weight'][2], data['broadener_weight'][2], data['d'][2]/2+data['broadener_d'][2]/2)\n",
    "            molecules[key+'_'+broadener]['broadness_jeanna'] = broadness_jeanna\n",
    "            #print(molecules[key+'_'+broadener])\n",
    "            #molecules[key] = molecules[key].drop(columns=['air_weight'])#symmetry\n",
    "            #print(molecules[key])\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-06T15:04:49.347399Z",
     "start_time": "2024-09-06T15:04:49.343875Z"
    }
   },
   "id": "c34fc3dc29a8f32a"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'C2H2_self'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/rq/8prqh34x6vj4hqbm2_ssk_ch0000gn/T/ipykernel_80545/1015253655.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmolecules\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'C2H2_self'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmolecules\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'C2H2_CO2'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmolecules\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'C2H2_self'"
     ]
    }
   ],
   "source": [
    "print(molecules['C2H2_self'].columns)\n",
    "print(molecules['C2H2_CO2'].columns)\n",
    "print(len(molecules))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-06T15:04:49.525135Z",
     "start_time": "2024-09-06T15:04:49.351084Z"
    }
   },
   "id": "b37af9363b205ddc"
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage \"good\" data =\n",
      "0.22262681199767262\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "\n",
    "\n",
    "###  Investigate data\n",
    "\n",
    "# Work out the average error codes of each molecule.  Count how many of each error code, and how many points there are\n",
    "\n",
    "# list of molecule names\n",
    "molecule_names = []\n",
    "# list of the proportion of data that is code 3 or above for each molecule\n",
    "error_2s = []\n",
    "# list of the number of points each molecule contains\n",
    "datapoints = []\n",
    "\n",
    "\n",
    "for key, data in molecules.items():\n",
    "    molecule, broadener = key.split('_')\n",
    "    molecule_names.append(key)\n",
    "    x = data['gamma-err']\n",
    "    # array of a 2s, b 3s, c 4s, etc...\n",
    "    y = x.value_counts().sort_index()\n",
    "\n",
    "    # if there is data classed as 2 or below, class it as 'bad'\n",
    "    if 2 in y.index:\n",
    "        error_2s.append(1-y.cumsum()[2]/x.count())\n",
    "        datapoints.append(x.count())\n",
    "    # otherwise all data is 'good'\n",
    "    elif 1 in y.index:\n",
    "        error_2s.append(1-y.cumsum()[1]/x.count())\n",
    "        datapoints.append(x.count())\n",
    "\n",
    "    elif 0 in y.index:\n",
    "        error_2s.append(1-y.cumsum()[0]/x.count())\n",
    "        datapoints.append(x.count())\n",
    "    else:\n",
    "        error_2s.append(1-0)\n",
    "        datapoints.append(x.count())\n",
    "        \n",
    "        \n",
    "\n",
    "# Print out how much data there is which is 'good' (out of every datapoint)\n",
    "e2 = np.array(error_2s)\n",
    "dat = np.array(datapoints)\n",
    "good_data = e2*dat\n",
    "print('percentage \"good\" data =')\n",
    "print(sum(good_data)/sum(dat))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T08:26:04.460020Z",
     "start_time": "2024-03-13T08:26:04.215592Z"
    }
   },
   "id": "cd0d58581cc5bc3d"
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [],
   "source": [
    "\n",
    "### Prepare data for use\n",
    "\n",
    "'''\n",
    "# reset molecules so that all molecules have the same number of points\n",
    "for molecule, data in molecules.items():\n",
    "    # normalise the amount of data compared to the molecule with the most data (SO2)\n",
    "    points = 549424//len(data)\n",
    "    #print(len(data))\n",
    "    # repeat data n times, until each has roughly the same amount of data\n",
    "    data = pd.concat([data]*points)\n",
    "    # assign data back to dictionar\n",
    "    molecules[molecule] = data\n",
    "'''\n",
    "\n",
    "# weight data by error code, currently error code = weighting\n",
    "for key, data in molecules.items():\n",
    "    molecule, broadener = key.split('_')\n",
    "\n",
    "    # take weight as gamma-air-err\n",
    "    #data=data.sample(frac=1)\n",
    "    weight = data['gamma-err']\n",
    "    # Give helpful weightings\n",
    "    # reweight 0 to tiny, because 0 gives /0 error\n",
    "    \n",
    "    if molecule in ['COCl2', 'C2H2', 'HOBr', 'H2', 'CH3OH', 'HCOOH', 'HOCl', 'COF2', 'HC3N']:\n",
    "        weight = (1/1000000000)**2\n",
    "    elif molecule in ['HNO3', 'O2', 'CS', 'ClO', 'CH3CN', 'H2O2', 'C2H4', 'O3']:\n",
    "        weight = weight.replace(0, (1/500000000)**2)    # 0  ~~~  unreported or unavailable\n",
    "        weight = weight.replace(1, (1/20000000)**2)    # 1  ~~~  Default or constant\n",
    "        weight = weight.replace(2, (1/1000000)**2)    # 2  ~~~  Average or estimate\n",
    "        weight = weight.replace(3, (1/50)**2)     # 3  ~~~  err >= 20 %              50\n",
    "        weight = weight.replace(4, (1/15)**2)     # 4  ~~~  20 >= err >= 10 %        15\n",
    "        weight = weight.replace(5, (1/10)**2)    # 5  ~~~  10 >= err >= 5 %         7.5\n",
    "        weight = weight.replace(6, (1/10)**2)    # 6  ~~~  5 >= err >= 2 %          3.5\n",
    "        weight = weight.replace(7, (1/10)**2)    # 7  ~~~  2 >= err >= 1 %          1.5\n",
    "        weight = weight.replace(8, (1/10)**2)    # 8  ~~~  err <= 1 %               0.5\n",
    "    elif molecule in ['C2H6']:\n",
    "        weight = weight.replace(0, (1/500000)**2)    # 0  ~~~  unreported or unavailable\n",
    "        weight = weight.replace(1, (1/20000)**2)    # 1  ~~~  Default or constant\n",
    "        weight = weight.replace(2, (1/1000)**2)    # 2  ~~~  Average or estimate\n",
    "        weight = weight.replace(3, (1/50)**2)     # 3  ~~~  err >= 20 %              50\n",
    "        weight = weight.replace(4, (1/15000000000000)**2)     # 4  ~~~  20 >= err >= 10 %        15\n",
    "        weight = weight.replace(5, (1/10)**2)    # 5  ~~~  10 >= err >= 5 %         7.5\n",
    "        weight = weight.replace(6, (1/10)**2)    # 6  ~~~  5 >= err >= 2 %          3.5\n",
    "        weight = weight.replace(7, (1/10)**2)    # 7  ~~~  2 >= err >= 1 %          1.5\n",
    "        weight = weight.replace(8, (1/10)**2)    # 8  ~~~  err <= 1 %               0.5\n",
    "    elif molecule in ['SO2']:\n",
    "        weight = weight.replace(0, (1/500000000)**2)    # 0  ~~~  unreported or unavailable\n",
    "        weight = weight.replace(1, (1/20000000)**2)    # 1  ~~~  Default or constant\n",
    "        weight = weight.replace(2, (1/1000000)**2)    # 2  ~~~  Average or estimate\n",
    "        weight = weight.replace(3, (1/50000)**2)     # 3  ~~~  err >= 20 %              50\n",
    "        weight = weight.replace(4, (1/15)**2)     # 4  ~~~  20 >= err >= 10 %        15\n",
    "        weight = weight.replace(5, (1/10)**2)    # 5  ~~~  10 >= err >= 5 %         7.5\n",
    "        weight = weight.replace(6, (1/10)**2)    # 6  ~~~  5 >= err >= 2 %          3.5\n",
    "        weight = weight.replace(7, (1/10)**2)    # 7  ~~~  2 >= err >= 1 %          1.5\n",
    "        weight = weight.replace(8, (1/10)**2)    # 8  ~~~  err <= 1 %               0.5\n",
    "    else:\n",
    "        weight = weight.replace(0, (1/500000)**2)    # 0  ~~~  unreported or unavailable\n",
    "        weight = weight.replace(1, (1/20000)**2)    # 1  ~~~  Default or constant\n",
    "        weight = weight.replace(2, (1/1000)**2)    # 2  ~~~  Average or estimate\n",
    "        weight = weight.replace(3, (1/50)**2)     # 3  ~~~  err >= 20 %              50\n",
    "        weight = weight.replace(4, (1/15)**2)     # 4  ~~~  20 >= err >= 10 %        15\n",
    "        weight = weight.replace(5, (1/10)**2)    # 5  ~~~  10 >= err >= 5 %         7.5\n",
    "        weight = weight.replace(6, (1/10)**2)    # 6  ~~~  5 >= err >= 2 %          3.5\n",
    "        weight = weight.replace(7, (1/10)**2)    # 7  ~~~  2 >= err >= 1 %          1.5\n",
    "        weight = weight.replace(8, (1/10)**2)    # 8  ~~~  err <= 1 %               0.5\n",
    "    \n",
    "    # reassign weight into dictionary\n",
    "    molecules[key]['gamma-err'] = weight\n",
    "    \n",
    "\n",
    "\n",
    "    #print(molecule)\n",
    "    #print(len(data))\n",
    "    datas=[]\n",
    "    if isinstance(weight, float):\n",
    "        points=0\n",
    "        data = data.sample(frac=points, replace=True)\n",
    "        molecules[key] = data\n",
    "    else:\n",
    "        for weight_value in weight.unique():\n",
    "            amount_data = len(weight[weight==weight_value])\n",
    "            #print(weight_value)\n",
    "            #print(amount_data)\n",
    "            fraction = amount_data / len(data)\n",
    "            points = 60000000*weight_value*fraction/len(data)\n",
    "            #print(fraction)\n",
    "            #print('fraction kept = '+str(points)) \n",
    "            #print()\n",
    "            datas.append(data[weight==weight_value].sample(frac=points, replace=True))\n",
    "    \n",
    "        data = pd.concat(datas)\n",
    "    # assign data back to dictionary\n",
    "    molecules[key] = data\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "    '''average_weight = np.mean(weight)\n",
    "    points = 600000000*average_weight/len(data)\n",
    "    print(molecule)\n",
    "    print('avg_weighting = '+str(average_weight))\n",
    "    print('fraction kept = '+str(points)) \n",
    "    print()\n",
    "    data = data.sample(frac=points, replace=True)\n",
    "    #data = pd.concat([data]*points)\n",
    "    # assign data back to dictionary\n",
    "    molecules[molecule] = data\n",
    "    '''\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T08:26:11.873282Z",
     "start_time": "2024-03-13T08:26:04.483914Z"
    }
   },
   "id": "1cf79391124beb19"
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['CS2_self', 'CS2_CO2', 'CS2_H2O', 'CS2_H2', 'CS_self', 'CS_CO2', 'CS_H2O', 'CS_H2', 'COF2_self', 'COF2_CO2', 'COF2_H2O', 'COF2_H2', 'NO2_self', 'NO2_CO2', 'NO2_H2O', 'NO2_H2', 'O3_self', 'O3_CO2', 'O3_H2O', 'O3_H2', 'HBr_self', 'HBr_CO2', 'HBr_H2O', 'HBr_H2', 'O2_self', 'O2_CO2', 'O2_H2O', 'O2_H2', 'PH3_self', 'PH3_CO2', 'PH3_H2O', 'PH3_H2', 'HNO3_self', 'HNO3_CO2', 'HNO3_H2O', 'HNO3_H2', 'N2_self', 'N2_CO2', 'N2_H2O', 'N2_H2', 'HO2_self', 'HO2_CO2', 'HO2_H2O', 'HO2_H2', 'SO2_self', 'SO2_CO2', 'SO2_H2O', 'SO2_H2', 'COCl2_self', 'COCl2_CO2', 'COCl2_H2O', 'COCl2_H2', 'HC3N_self', 'HC3N_CO2', 'HC3N_H2O', 'HC3N_H2', 'GeH4_self', 'GeH4_CO2', 'GeH4_H2O', 'GeH4_H2', 'CH3OH_self', 'CH3OH_CO2', 'CH3OH_H2O', 'CH3OH_H2', 'OCS_self', 'OCS_CO2', 'OCS_H2O', 'OCS_H2', 'HOBr_self', 'HOBr_CO2', 'HOBr_H2O', 'HOBr_H2', 'CH3I_self', 'CH3I_CO2', 'CH3I_H2O', 'CH3I_H2', 'H2O2_self', 'H2O2_CO2', 'H2O2_H2O', 'H2O2_H2', 'H2S_self', 'H2S_CO2', 'H2S_H2O', 'H2S_H2', 'CH3Cl_self', 'CH3Cl_CO2', 'CH3Cl_H2O', 'CH3Cl_H2', 'HF_self', 'HF_CO2', 'HF_H2O', 'HF_H2', 'NH3_self', 'NH3_CO2', 'NH3_H2O', 'NH3_H2', 'H2_self', 'H2_CO2', 'H2_H2O', 'H2_H2', 'CH3CN_self', 'CH3CN_CO2', 'CH3CN_H2O', 'CH3CN_H2', 'SO_self', 'SO_CO2', 'SO_H2O', 'SO_H2', 'HI_self', 'HI_CO2', 'HI_H2O', 'HI_H2', 'ClO_self', 'ClO_CO2', 'ClO_H2O', 'ClO_H2', 'CH3Br_self', 'CH3Br_CO2', 'CH3Br_H2O', 'CH3Br_H2', 'C2N2_self', 'C2N2_CO2', 'C2N2_H2O', 'C2N2_H2', 'N2O_self', 'N2O_CO2', 'N2O_H2O', 'N2O_H2', 'HOCl_self', 'HOCl_CO2', 'HOCl_H2O', 'HOCl_H2', 'H2CO_self', 'H2CO_CO2', 'H2CO_H2O', 'H2CO_H2', 'CO2_self', 'CO2_CO2', 'CO2_H2O', 'CO2_H2', 'H2O_self', 'H2O_CO2', 'H2O_H2O', 'H2O_H2', 'CH3F_self', 'CH3F_CO2', 'CH3F_H2O', 'CH3F_H2', 'HCOOH_self', 'HCOOH_CO2', 'HCOOH_H2O', 'HCOOH_H2', 'C2H2_self', 'C2H2_CO2', 'C2H2_H2O', 'C2H2_H2', 'CO_self', 'CO_CO2', 'CO_H2O', 'CO_H2', 'C2H4_self', 'C2H4_CO2', 'C2H4_H2O', 'C2H4_H2', 'OH_self', 'OH_CO2', 'OH_H2O', 'OH_H2', 'HCl_self', 'HCl_CO2', 'HCl_H2O', 'HCl_H2', 'C4H2_self', 'C4H2_CO2', 'C4H2_H2O', 'C4H2_H2', 'HCN_self', 'HCN_CO2', 'HCN_H2O', 'HCN_H2', 'C2H6_self', 'C2H6_CO2', 'C2H6_H2O', 'C2H6_H2', 'CH4_self', 'CH4_CO2', 'CH4_H2O', 'CH4_H2', 'NO_self', 'NO_CO2', 'NO_H2O', 'NO_H2'])\n"
     ]
    }
   ],
   "source": [
    "print(molecules.keys())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T08:26:11.874937Z",
     "start_time": "2024-03-13T08:26:11.872410Z"
    }
   },
   "id": "eb4ca5b374411e17"
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "'''\n",
    "# reset molecules so that all molecules have the same number of points\n",
    "for molecule, data in molecules.items():\n",
    "    # normalise the amount of data compared to the molecule with the most data (SO2)\n",
    "    print(len(data))\n",
    "    weight = 950863/len(data)\n",
    "    #weight = 549424/len(data)\n",
    "    # assign data back to dictionary\n",
    "    molecules[molecule]['gamma_air-err'] = data['gamma_air-err']*weight\n",
    "'''\n",
    "\n",
    "import random\n",
    "keys = list(molecules.items())\n",
    "random.seed(41)\n",
    "random.shuffle(keys)\n",
    "molecules = dict(keys)\n",
    "    \n",
    "# Dictionary of molecules, and test/training data\n",
    "molecule_list = {}\n",
    "\n",
    "# collect 'training data' from all other molecules, except the labelled one\n",
    "i=0\n",
    "# collect 'training data' from all other molecules, except the labelled one\n",
    "for molecule in molecules:\n",
    "    if not i%20:\n",
    "        print(i)\n",
    "        # molecule is being tested\n",
    "        test_data = {k: molecules[k] for k in list(molecules)[i:i+20]}\n",
    "        #data_test = molecules[molecule]\n",
    "        # take test molecule out of dictionary\n",
    "        train_data = set(molecules) - set(test_data)\n",
    "        #print(test_data)\n",
    "        #print(test_data.keys(), train_data)\n",
    "        # dictionary of molecules in test data\n",
    "        train_data = {k: molecules[k] for k in train_data}\n",
    "        \n",
    "        # All test data in one dataframe\n",
    "        data_test = pd.concat([test_data[k] for k in test_data])\n",
    "\n",
    "        # Take all train data into one dataframe\n",
    "        data_train = pd.concat([train_data[k] for k in train_data])\n",
    "        \n",
    "        # add data into molecule_list dictionary\n",
    "        moles = ','.join(list(test_data.keys()))\n",
    "        molecule_list[moles] = [data_train, data_test]\n",
    "        \n",
    "    i+=1\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#print(molecule_list)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T08:26:41.091895Z",
     "start_time": "2024-03-13T08:26:12.297353Z"
    }
   },
   "id": "69965ee2dd1479ef"
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "'''\n",
    "# concatenate all dataframes once\n",
    "data_train_all = pd.concat(list(molecules.values()))\n",
    "\n",
    "\n",
    "molecule_list = {}\n",
    "\n",
    "a = []\n",
    "for i, data_test in enumerate(list(molecules.values())):\n",
    "    # slice the concatenated dataframe\n",
    "    data_train = pd.concat([data_train_all.iloc[:i*len(molecule_list)], data_train_all.iloc[(i+1)*len(molecule_list):]])\n",
    "\n",
    "    molecule_list[list(molecules.keys())[i]] = [data_train, data_test]\n",
    "\n",
    "\n",
    "'''\n",
    "'''\n",
    "# Dictionary of molecules, and test/training data\n",
    "molecule_list = {}\n",
    "\n",
    "# collect 'training data' from all other molecules, except the labelled one\n",
    "for molecule in molecules:\n",
    "    # molecule is being tested\n",
    "    data_test = molecules[molecule].to_numpy()\n",
    "    # take test molecule out of dictionary\n",
    "    train_data = {}\n",
    "    for k in molecules:\n",
    "        if k != molecule:\n",
    "            train_data[k] = molecules[k].to_numpy()\n",
    "\n",
    "    data_train = np.concatenate([train_data[k] for k in train_data])\n",
    "    molecule_list[molecule] = [data_train, data_test]\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "def what_is_error(weight):\n",
    "    if weight == (1/500000)**2:\n",
    "        weight = 'Unavailable'\n",
    "    if weight == (1/20000)**2:\n",
    "        weight = 'Constant'\n",
    "    if weight == (1/1000)**2:\n",
    "        weight = 'Estimate'\n",
    "    if weight == (1/50)**2:\n",
    "        weight = '>20%'\n",
    "    if weight == (1/15)**2:\n",
    "        weight = '20> >10%'\n",
    "    if weight == (1/10)**2:\n",
    "        weight = '<10%'\n",
    "    return weight\n",
    "\n",
    "\n",
    "\n",
    "plot_data_list = []"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T08:26:41.217264Z",
     "start_time": "2024-03-13T08:26:41.214966Z"
    }
   },
   "id": "c61fdd8e26ae775b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1785734\n",
      "11447098\n",
      "lets train!!!\n",
      "building tree 1 of 10\n",
      "building tree 2 of 10\n",
      "building tree 3 of 10\n",
      "building tree 4 of 10\n",
      "building tree 5 of 10\n",
      "building tree 6 of 10\n",
      "building tree 7 of 10\n",
      "building tree 8 of 10\n",
      "building tree 9 of 10\n",
      "building tree 10 of 10\n",
      "[Voting] ................... (4 of 5) Processing forest, total=   0.6s\n",
      "building tree 1 of 10\n",
      "building tree 2 of 10\n",
      "building tree 3 of 10\n",
      "building tree 4 of 10\n",
      "building tree 5 of 10\n",
      "building tree 6 of 10\n",
      "building tree 7 of 10\n",
      "building tree 8 of 10\n",
      "building tree 9 of 10\n",
      "building tree 10 of 10\n",
      "[Voting] ................... (4 of 5) Processing forest, total=   0.3s\n",
      "[Voting] ...................... (3 of 5) Processing svr, total=   0.5s\n",
      "[Voting] ...................... (3 of 5) Processing svr, total=   0.4s\n",
      "[Voting] ...................... (2 of 5) Processing ada, total=   0.3s\n",
      "[Voting] ..................... (1 of 5) Processing hist, total=   0.5s\n",
      "Iteration 1, loss = 0.04263187\n",
      "Iteration 2, loss = 0.00787375\n",
      "Iteration 3, loss = 0.00511283\n",
      "Iteration 4, loss = 0.00407901\n",
      "Iteration 5, loss = 0.00349852\n",
      "Iteration 6, loss = 0.00314347\n",
      "Iteration 7, loss = 0.00292984\n",
      "Iteration 8, loss = 0.00273737\n",
      "Iteration 9, loss = 0.00258627\n",
      "Iteration 10, loss = 0.00248476\n",
      "Iteration 11, loss = 0.00239240\n",
      "Iteration 12, loss = 0.00229459\n",
      "Iteration 13, loss = 0.00222042\n",
      "Iteration 14, loss = 0.00217481\n",
      "Iteration 15, loss = 0.00212707\n",
      "Iteration 16, loss = 0.00208659\n",
      "Iteration 17, loss = 0.00204540\n",
      "Iteration 18, loss = 0.00199003\n",
      "Iteration 19, loss = 0.00194441\n",
      "Iteration 20, loss = 0.00191631\n",
      "Iteration 21, loss = 0.00187868\n",
      "Iteration 22, loss = 0.00184544\n",
      "Iteration 23, loss = 0.00182420\n",
      "Iteration 24, loss = 0.00180849\n",
      "Iteration 25, loss = 0.00176752\n",
      "Iteration 26, loss = 0.00173706\n",
      "Iteration 27, loss = 0.00171333\n",
      "Iteration 28, loss = 0.00168964\n",
      "Iteration 29, loss = 0.00166470\n",
      "Iteration 30, loss = 0.00163332\n",
      "Iteration 31, loss = 0.00160326\n",
      "Iteration 32, loss = 0.00159107\n",
      "Iteration 33, loss = 0.00156544\n",
      "Iteration 34, loss = 0.00153923\n",
      "Iteration 35, loss = 0.00151313\n",
      "Iteration 36, loss = 0.00150380\n",
      "Iteration 37, loss = 0.00147782\n",
      "Iteration 38, loss = 0.00145339\n",
      "Iteration 39, loss = 0.00142225\n",
      "Iteration 40, loss = 0.00140794\n",
      "Iteration 41, loss = 0.00138120\n",
      "Iteration 42, loss = 0.00137021\n",
      "Iteration 43, loss = 0.00135502\n",
      "Iteration 44, loss = 0.00134217\n",
      "Iteration 45, loss = 0.00132314\n",
      "Iteration 46, loss = 0.00130082\n",
      "Iteration 47, loss = 0.00129877\n",
      "Iteration 48, loss = 0.00127389\n",
      "Iteration 49, loss = 0.00125453\n",
      "Iteration 50, loss = 0.00123720\n",
      "Iteration 51, loss = 0.00122845\n",
      "Iteration 52, loss = 0.00119630\n",
      "Iteration 53, loss = 0.00118225\n",
      "Iteration 54, loss = 0.00117089\n",
      "Iteration 55, loss = 0.00115927\n",
      "Iteration 56, loss = 0.00114057\n",
      "Iteration 57, loss = 0.00113332\n",
      "Iteration 58, loss = 0.00110588\n",
      "Iteration 59, loss = 0.00109714\n",
      "Iteration 60, loss = 0.00109450\n",
      "Training loss did not improve more than tol=0.000010 for 1 consecutive epochs. Stopping.\n",
      "[Voting] ...................... (5 of 5) Processing mlp, total=   1.7s\n",
      "building tree 1 of 10\n",
      "building tree 2 of 10\n",
      "building tree 3 of 10\n",
      "building tree 4 of 10\n",
      "building tree 5 of 10\n",
      "building tree 6 of 10\n",
      "building tree 7 of 10\n",
      "building tree 8 of 10\n",
      "building tree 9 of 10\n",
      "building tree 10 of 10\n",
      "[Voting] ................... (4 of 5) Processing forest, total=   0.3s\n",
      "[Voting] ..................... (1 of 5) Processing hist, total=   0.8s\n",
      "[Voting] ..................... (1 of 5) Processing hist, total=   0.5s\n",
      "Iteration 1, loss = 0.03897724\n",
      "Iteration 2, loss = 0.00721472\n",
      "Iteration 3, loss = 0.00463941\n",
      "Iteration 4, loss = 0.00371349\n",
      "Iteration 5, loss = 0.00318070\n",
      "Iteration 6, loss = 0.00285394\n",
      "Iteration 7, loss = 0.00263695\n",
      "Iteration 8, loss = 0.00245433\n",
      "Iteration 9, loss = 0.00233897\n",
      "Iteration 10, loss = 0.00224128\n",
      "Iteration 11, loss = 0.00216032\n",
      "Iteration 12, loss = 0.00208880\n",
      "Iteration 13, loss = 0.00202188\n",
      "Iteration 14, loss = 0.00197015\n",
      "Iteration 15, loss = 0.00194011\n",
      "Iteration 16, loss = 0.00189069\n",
      "Iteration 17, loss = 0.00186053\n",
      "Iteration 18, loss = 0.00182883\n",
      "Iteration 19, loss = 0.00179281\n",
      "Iteration 20, loss = 0.00175859\n",
      "Iteration 21, loss = 0.00173177\n",
      "Iteration 22, loss = 0.00169113\n",
      "Iteration 23, loss = 0.00166756\n",
      "Iteration 24, loss = 0.00164202\n",
      "Iteration 25, loss = 0.00162422\n",
      "Iteration 26, loss = 0.00159898\n",
      "Iteration 27, loss = 0.00158237\n",
      "Iteration 28, loss = 0.00155804\n",
      "Iteration 29, loss = 0.00152864\n",
      "Iteration 30, loss = 0.00151573\n",
      "Iteration 31, loss = 0.00148835\n",
      "Iteration 32, loss = 0.00146453\n",
      "Iteration 33, loss = 0.00145269\n",
      "Iteration 34, loss = 0.00142272\n",
      "Iteration 35, loss = 0.00142053\n",
      "Iteration 36, loss = 0.00138637\n",
      "Iteration 37, loss = 0.00137558\n",
      "Iteration 38, loss = 0.00135297\n",
      "Iteration 39, loss = 0.00133749\n",
      "Iteration 40, loss = 0.00131636\n",
      "Iteration 41, loss = 0.00129559\n",
      "Iteration 42, loss = 0.00128217\n",
      "Iteration 43, loss = 0.00126622\n",
      "Iteration 44, loss = 0.00125678\n",
      "Iteration 45, loss = 0.00121811\n",
      "Iteration 46, loss = 0.00121067\n",
      "Iteration 47, loss = 0.00119972\n",
      "Iteration 48, loss = 0.00118436\n",
      "Iteration 49, loss = 0.00118078\n",
      "Iteration 50, loss = 0.00114934\n",
      "Iteration 51, loss = 0.00113730\n",
      "Iteration 52, loss = 0.00111828\n",
      "Iteration 53, loss = 0.00110833\n",
      "Iteration 54, loss = 0.00108650\n",
      "Iteration 55, loss = 0.00107040\n",
      "Iteration 56, loss = 0.00105794\n",
      "Iteration 57, loss = 0.00104134\n",
      "Iteration 58, loss = 0.00105065\n",
      "Iteration 59, loss = 0.00103460\n",
      "Training loss did not improve more than tol=0.000010 for 1 consecutive epochs. Stopping.\n",
      "[Voting] ...................... (5 of 5) Processing mlp, total=   1.4s\n",
      "[Voting] ...................... (2 of 5) Processing ada, total=   0.5s\n",
      "[Voting] ...................... (2 of 5) Processing ada, total=   0.3s\n",
      "[Voting] ...................... (3 of 5) Processing svr, total=   0.4s\n",
      "[Voting] ..................... (1 of 5) Processing hist, total=   0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Voting] ..................... (1 of 5) Processing hist, total=   0.4s\n",
      "Iteration 1, loss = 0.04481496\n",
      "Iteration 2, loss = 0.00769295\n",
      "Iteration 3, loss = 0.00500346\n",
      "Iteration 4, loss = 0.00399523\n",
      "Iteration 5, loss = 0.00344658\n",
      "Iteration 6, loss = 0.00306830\n",
      "Iteration 7, loss = 0.00278538\n",
      "Iteration 8, loss = 0.00259211\n",
      "Iteration 9, loss = 0.00243434\n",
      "Iteration 10, loss = 0.00233524\n",
      "Iteration 11, loss = 0.00223212\n",
      "Iteration 12, loss = 0.00216518\n",
      "Iteration 13, loss = 0.00208514\n",
      "Iteration 14, loss = 0.00203588\n",
      "Iteration 15, loss = 0.00198505\n",
      "Iteration 16, loss = 0.00194006\n",
      "Iteration 17, loss = 0.00189820\n",
      "Iteration 18, loss = 0.00187025\n",
      "Iteration 19, loss = 0.00183887\n",
      "Iteration 20, loss = 0.00181227\n",
      "Iteration 21, loss = 0.00178027\n",
      "Iteration 22, loss = 0.00175353\n",
      "Iteration 23, loss = 0.00171243\n",
      "Iteration 24, loss = 0.00169842\n",
      "Iteration 25, loss = 0.00167511\n",
      "Iteration 26, loss = 0.00164127\n",
      "Iteration 27, loss = 0.00161744\n",
      "Iteration 28, loss = 0.00160309\n",
      "Iteration 29, loss = 0.00157997\n",
      "Iteration 30, loss = 0.00156273\n",
      "Iteration 31, loss = 0.00152553\n",
      "Iteration 32, loss = 0.00151596\n",
      "Iteration 33, loss = 0.00148944\n",
      "Iteration 34, loss = 0.00148220\n",
      "Iteration 35, loss = 0.00144967\n",
      "Iteration 36, loss = 0.00143816\n",
      "Iteration 37, loss = 0.00140442\n",
      "Iteration 38, loss = 0.00138210\n",
      "Iteration 39, loss = 0.00136373\n",
      "Iteration 40, loss = 0.00133807\n",
      "Iteration 41, loss = 0.00132269\n",
      "Iteration 42, loss = 0.00130432\n",
      "Iteration 43, loss = 0.00128654\n",
      "Iteration 44, loss = 0.00126482\n",
      "Iteration 45, loss = 0.00124986\n",
      "Iteration 46, loss = 0.00123775\n",
      "Iteration 47, loss = 0.00121336\n",
      "Iteration 48, loss = 0.00120130\n",
      "Iteration 49, loss = 0.00118031\n",
      "Iteration 50, loss = 0.00117357\n",
      "Iteration 51, loss = 0.00115207\n",
      "Iteration 52, loss = 0.00115184\n",
      "Iteration 53, loss = 0.00112551\n",
      "Iteration 54, loss = 0.00110821\n",
      "Iteration 55, loss = 0.00108028\n",
      "Iteration 56, loss = 0.00106656\n",
      "Iteration 57, loss = 0.00104892\n",
      "Iteration 58, loss = 0.00103831\n",
      "Iteration 59, loss = 0.00102193\n",
      "Iteration 60, loss = 0.00100248\n",
      "Iteration 61, loss = 0.00100910\n",
      "Iteration 62, loss = 0.00098108\n",
      "Iteration 63, loss = 0.00097437\n",
      "Iteration 64, loss = 0.00095402\n",
      "Iteration 65, loss = 0.00093873\n",
      "Iteration 66, loss = 0.00094009\n",
      "Iteration 67, loss = 0.00091360\n",
      "Iteration 68, loss = 0.00089962\n",
      "Iteration 69, loss = 0.00088592\n",
      "Iteration 70, loss = 0.00089193\n",
      "Iteration 71, loss = 0.00085860\n",
      "Iteration 72, loss = 0.00084961\n",
      "Iteration 73, loss = 0.00083940\n",
      "Iteration 74, loss = 0.00082989\n",
      "Iteration 75, loss = 0.00081962\n",
      "Iteration 76, loss = 0.00081182\n",
      "Iteration 77, loss = 0.00080847\n",
      "Training loss did not improve more than tol=0.000010 for 1 consecutive epochs. Stopping.\n",
      "[Voting] ...................... (5 of 5) Processing mlp, total=   2.1s\n",
      "[Voting] ..................... (1 of 5) Processing hist, total=   0.4s\n",
      "building tree 1 of 10\n",
      "building tree 2 of 10\n",
      "building tree 3 of 10\n",
      "building tree 4 of 10\n",
      "building tree 5 of 10\n",
      "building tree 6 of 10\n",
      "building tree 7 of 10\n",
      "building tree 8 of 10\n",
      "building tree 9 of 10\n",
      "building tree 10 of 10\n",
      "[Voting] ................... (4 of 5) Processing forest, total=   0.3s\n",
      "building tree 1 of 10\n",
      "building tree 2 of 10\n",
      "building tree 3 of 10\n",
      "building tree 4 of 10\n",
      "building tree 5 of 10\n",
      "building tree 6 of 10\n",
      "building tree 7 of 10\n",
      "building tree 8 of 10\n",
      "building tree 9 of 10\n",
      "building tree 10 of 10\n",
      "[Voting] ................... (4 of 5) Processing forest, total=   0.4s\n",
      "[Voting] ...................... (3 of 5) Processing svr, total=   0.5s\n",
      "Iteration 1, loss = 0.04031779\n",
      "Iteration 2, loss = 0.00630702\n",
      "Iteration 3, loss = 0.00401119\n",
      "Iteration 4, loss = 0.00323654\n",
      "Iteration 5, loss = 0.00282408\n",
      "Iteration 6, loss = 0.00255439\n",
      "Iteration 7, loss = 0.00240948\n",
      "Iteration 8, loss = 0.00224846\n",
      "Iteration 9, loss = 0.00215022\n",
      "Iteration 10, loss = 0.00207124\n",
      "Iteration 11, loss = 0.00199199\n",
      "Iteration 12, loss = 0.00192951\n",
      "Iteration 13, loss = 0.00187091\n",
      "Iteration 14, loss = 0.00181386\n",
      "Iteration 15, loss = 0.00176269\n",
      "Iteration 16, loss = 0.00175586\n",
      "Iteration 17, loss = 0.00168571\n",
      "Iteration 18, loss = 0.00166068\n",
      "Iteration 19, loss = 0.00162358\n",
      "Iteration 20, loss = 0.00159461\n",
      "Iteration 21, loss = 0.00157525\n",
      "Iteration 22, loss = 0.00153578\n",
      "Iteration 23, loss = 0.00151341\n",
      "Iteration 24, loss = 0.00148133\n",
      "Iteration 25, loss = 0.00146425\n",
      "Iteration 26, loss = 0.00144493\n",
      "Iteration 27, loss = 0.00141959\n",
      "Iteration 28, loss = 0.00139783\n",
      "Iteration 29, loss = 0.00137309\n",
      "Iteration 30, loss = 0.00135865\n",
      "Iteration 31, loss = 0.00133591\n",
      "Iteration 32, loss = 0.00131390\n",
      "Iteration 33, loss = 0.00129333\n",
      "Iteration 34, loss = 0.00127561\n",
      "Iteration 35, loss = 0.00126745\n",
      "Iteration 36, loss = 0.00124204\n",
      "Iteration 37, loss = 0.00122107\n",
      "Iteration 38, loss = 0.00120644\n",
      "Iteration 39, loss = 0.00118598\n",
      "Iteration 40, loss = 0.00116967\n",
      "Iteration 41, loss = 0.00116657\n",
      "Iteration 42, loss = 0.00114521\n",
      "Iteration 43, loss = 0.00112649\n",
      "Iteration 44, loss = 0.00110690\n",
      "Iteration 45, loss = 0.00108804\n",
      "Iteration 46, loss = 0.00107320\n",
      "Iteration 47, loss = 0.00105076\n",
      "Iteration 48, loss = 0.00104180\n",
      "Iteration 49, loss = 0.00123459\n",
      "Training loss did not improve more than tol=0.000010 for 1 consecutive epochs. Stopping.\n",
      "[Voting] ...................... (5 of 5) Processing mlp, total=   1.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Voting] ...................... (3 of 5) Processing svr, total=   0.7s\n",
      "Iteration 1, loss = 0.04335570\n",
      "Iteration 2, loss = 0.00713367\n",
      "Iteration 3, loss = 0.00445959\n",
      "Iteration 4, loss = 0.00351923\n",
      "Iteration 5, loss = 0.00305078\n",
      "Iteration 6, loss = 0.00279855\n",
      "Iteration 7, loss = 0.00262803\n",
      "Iteration 8, loss = 0.00249594\n",
      "Iteration 9, loss = 0.00238249\n",
      "Iteration 10, loss = 0.00229770\n",
      "Iteration 11, loss = 0.00219991\n",
      "Iteration 12, loss = 0.00211255\n",
      "Iteration 13, loss = 0.00205265\n",
      "Iteration 14, loss = 0.00200179\n",
      "Iteration 15, loss = 0.00195837\n",
      "Iteration 16, loss = 0.00191711\n",
      "Iteration 17, loss = 0.00187672\n",
      "Iteration 18, loss = 0.00183810\n",
      "Iteration 19, loss = 0.00180175\n",
      "Iteration 20, loss = 0.00176669\n",
      "Iteration 21, loss = 0.00173442\n",
      "Iteration 22, loss = 0.00171028\n",
      "Iteration 23, loss = 0.00170894\n",
      "Iteration 24, loss = 0.00165305\n",
      "Iteration 25, loss = 0.00163181\n",
      "Iteration 26, loss = 0.00162080\n",
      "Iteration 27, loss = 0.00157957\n",
      "Iteration 28, loss = 0.00157396\n",
      "Iteration 29, loss = 0.00153247\n",
      "Iteration 30, loss = 0.00150506\n",
      "Iteration 31, loss = 0.00148477\n",
      "Iteration 32, loss = 0.00147628\n",
      "Iteration 33, loss = 0.00145189\n",
      "Iteration 34, loss = 0.00142007\n",
      "Iteration 35, loss = 0.00141552\n",
      "Iteration 36, loss = 0.00138660\n",
      "Iteration 37, loss = 0.00137970\n",
      "Iteration 38, loss = 0.00138972\n",
      "Training loss did not improve more than tol=0.000010 for 1 consecutive epochs. Stopping.\n",
      "[Voting] ...................... (5 of 5) Processing mlp, total=   1.0s\n",
      "building tree 1 of 10\n",
      "building tree 2 of 10\n",
      "building tree 3 of 10\n",
      "building tree 4 of 10\n",
      "building tree 5 of 10\n",
      "building tree 6 of 10\n",
      "building tree 7 of 10\n",
      "building tree 8 of 10\n",
      "building tree 9 of 10\n",
      "building tree 10 of 10\n",
      "[Voting] ................... (4 of 5) Processing forest, total=   0.3s\n",
      "building tree 1 of 10\n",
      "building tree 2 of 10\n",
      "building tree 3 of 10\n",
      "building tree 4 of 10\n",
      "building tree 5 of 10\n",
      "building tree 6 of 10\n",
      "building tree 7 of 10\n",
      "building tree 8 of 10\n",
      "building tree 9 of 10\n",
      "building tree 10 of 10\n",
      "[Voting] ................... (4 of 5) Processing forest, total=   0.3s\n",
      "building tree 1 of 10\n",
      "building tree 2 of 10\n",
      "building tree 3 of 10\n",
      "building tree 4 of 10\n",
      "building tree 5 of 10\n",
      "building tree 6 of 10\n",
      "building tree 7 of 10\n",
      "building tree 8 of 10\n",
      "building tree 9 of 10\n",
      "building tree 10 of 10\n",
      "[Voting] ................... (4 of 5) Processing forest, total=   0.3s\n",
      "Iteration 1, loss = 0.04539252\n",
      "Iteration 2, loss = 0.00798143\n",
      "Iteration 3, loss = 0.00500539\n",
      "Iteration 4, loss = 0.00393138\n",
      "Iteration 5, loss = 0.00346729\n",
      "Iteration 6, loss = 0.00321819\n",
      "Iteration 7, loss = 0.00291314\n",
      "Iteration 8, loss = 0.00275378\n",
      "Iteration 9, loss = 0.00257764\n",
      "Iteration 10, loss = 0.00246284\n",
      "Iteration 11, loss = 0.00237686\n",
      "Iteration 12, loss = 0.00231814\n",
      "Iteration 13, loss = 0.00223775\n",
      "Iteration 14, loss = 0.00214890\n",
      "Iteration 15, loss = 0.00209954\n",
      "Iteration 16, loss = 0.00209681\n",
      "Iteration 17, loss = 0.00199035\n",
      "Iteration 18, loss = 0.00195021\n",
      "Iteration 19, loss = 0.00189919\n",
      "Iteration 20, loss = 0.00185237\n",
      "Iteration 21, loss = 0.00180740\n",
      "Iteration 22, loss = 0.00176970\n",
      "Iteration 23, loss = 0.00176246\n",
      "Iteration 24, loss = 0.00171224\n",
      "Iteration 25, loss = 0.00169438\n",
      "Iteration 26, loss = 0.00164195\n",
      "Iteration 27, loss = 0.00162315\n",
      "Iteration 28, loss = 0.00159292\n",
      "Iteration 29, loss = 0.00157227\n",
      "Iteration 30, loss = 0.00154688\n",
      "Iteration 31, loss = 0.00165261\n",
      "Iteration 32, loss = 0.00149485\n",
      "Iteration 33, loss = 0.00146117\n",
      "Iteration 34, loss = 0.00142548\n",
      "Iteration 35, loss = 0.00140901\n",
      "Iteration 36, loss = 0.00137864\n",
      "Iteration 37, loss = 0.00135179\n",
      "Iteration 38, loss = 0.00133016\n",
      "Iteration 39, loss = 0.00130608\n",
      "Iteration 40, loss = 0.00128313\n",
      "Iteration 41, loss = 0.00163288\n",
      "Iteration 42, loss = 0.00123495\n",
      "Iteration 43, loss = 0.00121238\n",
      "Iteration 44, loss = 0.00119476\n",
      "Iteration 45, loss = 0.00117493\n",
      "Iteration 46, loss = 0.00117841\n",
      "Iteration 47, loss = 0.00116145\n",
      "Iteration 48, loss = 0.00112989\n",
      "Iteration 49, loss = 0.00109890\n",
      "Iteration 50, loss = 0.00110805\n",
      "Iteration 51, loss = 0.00106587\n",
      "Iteration 52, loss = 0.00105602\n",
      "Iteration 53, loss = 0.00103234\n",
      "Iteration 54, loss = 0.00105327\n",
      "Iteration 55, loss = 0.00102084\n",
      "Iteration 56, loss = 0.00110632\n",
      "Iteration 57, loss = 0.00097481\n",
      "Iteration 58, loss = 0.00102395\n",
      "Iteration 59, loss = 0.00093786\n",
      "Iteration 60, loss = 0.00099229\n",
      "Iteration 61, loss = 0.00092204\n",
      "Iteration 62, loss = 0.00089476\n",
      "Iteration 63, loss = 0.00088827\n",
      "Iteration 64, loss = 0.00118047\n",
      "Training loss did not improve more than tol=0.000010 for 1 consecutive epochs. Stopping.\n",
      "[Voting] ...................... (5 of 5) Processing mlp, total=   1.6s\n",
      "[Voting] ...................... (2 of 5) Processing ada, total=   0.5s\n",
      "building tree 1 of 10\n",
      "building tree 2 of 10\n",
      "building tree 3 of 10\n",
      "building tree 4 of 10\n",
      "building tree 5 of 10\n",
      "building tree 6 of 10\n",
      "building tree 7 of 10\n",
      "building tree 8 of 10\n",
      "building tree 9 of 10\n",
      "building tree 10 of 10\n",
      "[Voting] ................... (4 of 5) Processing forest, total=   0.4s\n",
      "[Voting] ..................... (1 of 5) Processing hist, total=   2.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Voting] ...................... (2 of 5) Processing ada, total=   0.7s\n",
      "building tree 1 of 10\n",
      "building tree 2 of 10\n",
      "building tree 3 of 10\n",
      "building tree 4 of 10\n",
      "building tree 5 of 10\n",
      "building tree 6 of 10\n",
      "building tree 7 of 10\n",
      "building tree 8 of 10\n",
      "building tree 9 of 10\n",
      "building tree 10 of 10\n",
      "[Voting] ................... (4 of 5) Processing forest, total=   0.3s\n",
      "Iteration 1, loss = 0.04334129\n",
      "Iteration 2, loss = 0.00697188\n",
      "Iteration 3, loss = 0.00480630\n",
      "Iteration 4, loss = 0.00393285\n",
      "Iteration 5, loss = 0.00349347\n",
      "Iteration 6, loss = 0.00317232\n",
      "Iteration 7, loss = 0.00292959\n",
      "Iteration 8, loss = 0.00273172\n",
      "Iteration 9, loss = 0.00257306\n",
      "Iteration 10, loss = 0.00244890\n",
      "Iteration 11, loss = 0.00237913\n",
      "Iteration 12, loss = 0.00231275\n",
      "Iteration 13, loss = 0.00220230\n",
      "Iteration 14, loss = 0.00215535\n",
      "Iteration 15, loss = 0.00207743\n",
      "Iteration 16, loss = 0.00203332\n",
      "Iteration 17, loss = 0.00199199\n",
      "Iteration 18, loss = 0.00196247\n",
      "Iteration 19, loss = 0.00191015\n",
      "Iteration 20, loss = 0.00187893\n",
      "Iteration 21, loss = 0.00186015\n",
      "Iteration 22, loss = 0.00178862\n",
      "Iteration 23, loss = 0.00174754\n",
      "Iteration 24, loss = 0.00171806\n",
      "Iteration 25, loss = 0.00170955\n",
      "Iteration 26, loss = 0.00166369\n",
      "Iteration 27, loss = 0.00164424\n",
      "Iteration 28, loss = 0.00162337\n",
      "Iteration 29, loss = 0.00158959\n",
      "Iteration 30, loss = 0.00155298\n",
      "Iteration 31, loss = 0.00153865\n",
      "Iteration 32, loss = 0.00152445\n",
      "Iteration 33, loss = 0.00150405\n",
      "Iteration 34, loss = 0.00145512\n",
      "Iteration 35, loss = 0.00144872\n",
      "Iteration 36, loss = 0.00140637\n",
      "Iteration 37, loss = 0.00141434\n",
      "Iteration 38, loss = 0.00138212\n",
      "Iteration 39, loss = 0.00137342\n",
      "Iteration 40, loss = 0.00134777\n",
      "Iteration 41, loss = 0.00131642\n",
      "Iteration 42, loss = 0.00130836\n",
      "Iteration 43, loss = 0.00129176\n",
      "Iteration 44, loss = 0.00126403\n",
      "Iteration 45, loss = 0.00124746\n",
      "Iteration 46, loss = 0.00124490\n",
      "Iteration 47, loss = 0.00124015\n",
      "Training loss did not improve more than tol=0.000010 for 1 consecutive epochs. Stopping.\n",
      "[Voting] ...................... (5 of 5) Processing mlp, total=   1.2s\n",
      "[Voting] ...................... (3 of 5) Processing svr, total=   0.4s\n",
      "[Voting] ..................... (1 of 5) Processing hist, total=   0.5s\n",
      "[Voting] ...................... (2 of 5) Processing ada, total=   0.8s\n",
      "building tree 1 of 10\n",
      "building tree 2 of 10\n",
      "building tree 3 of 10\n",
      "building tree 4 of 10\n",
      "building tree 5 of 10\n",
      "building tree 6 of 10\n",
      "building tree 7 of 10\n",
      "building tree 8 of 10\n",
      "building tree 9 of 10\n",
      "building tree 10 of 10\n",
      "[Voting] ................... (4 of 5) Processing forest, total=   3.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    3.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.04742902\n",
      "Iteration 2, loss = 0.00784081\n",
      "Iteration 3, loss = 0.00489347\n",
      "Iteration 4, loss = 0.00385634\n",
      "Iteration 5, loss = 0.00335650\n",
      "Iteration 6, loss = 0.00304568\n",
      "Iteration 7, loss = 0.00283363\n",
      "Iteration 8, loss = 0.00267393\n",
      "Iteration 9, loss = 0.00250503\n",
      "Iteration 10, loss = 0.00239354\n",
      "Iteration 11, loss = 0.00230254\n",
      "Iteration 12, loss = 0.00220503\n",
      "Iteration 13, loss = 0.00213623\n",
      "Iteration 14, loss = 0.00206146\n",
      "Iteration 15, loss = 0.00201902\n",
      "Iteration 16, loss = 0.00196324\n",
      "Iteration 17, loss = 0.00191958\n",
      "Iteration 18, loss = 0.00187606\n",
      "Iteration 19, loss = 0.00185202\n",
      "Iteration 20, loss = 0.00180995\n",
      "Iteration 21, loss = 0.00177683\n",
      "Iteration 22, loss = 0.00174720\n",
      "Iteration 23, loss = 0.00170266\n",
      "Iteration 24, loss = 0.00167422\n",
      "Iteration 25, loss = 0.00165641\n",
      "Iteration 26, loss = 0.00162778\n",
      "Iteration 27, loss = 0.00160432\n",
      "Iteration 28, loss = 0.00160456\n",
      "Iteration 29, loss = 0.00159061\n",
      "Iteration 30, loss = 0.00153720\n",
      "Iteration 31, loss = 0.00151192\n",
      "Iteration 32, loss = 0.00149498\n",
      "Iteration 33, loss = 0.00147787\n",
      "Iteration 34, loss = 0.00146131\n",
      "Iteration 35, loss = 0.00143043\n",
      "Iteration 36, loss = 0.00141130\n",
      "Iteration 37, loss = 0.00141196\n",
      "Iteration 38, loss = 0.00139086\n",
      "Iteration 39, loss = 0.00136734\n",
      "Iteration 40, loss = 0.00134949\n",
      "Iteration 41, loss = 0.00132353\n",
      "Iteration 42, loss = 0.00132102\n",
      "Iteration 43, loss = 0.00129583\n",
      "Iteration 44, loss = 0.00128283\n",
      "Iteration 45, loss = 0.00125526\n",
      "Iteration 46, loss = 0.00124509\n",
      "Iteration 47, loss = 0.00122666\n",
      "Iteration 48, loss = 0.00121116\n",
      "Iteration 49, loss = 0.00119185\n",
      "Iteration 50, loss = 0.00117187\n",
      "Iteration 51, loss = 0.00117379\n",
      "Iteration 52, loss = 0.00114800\n",
      "Iteration 53, loss = 0.00112942\n",
      "Iteration 54, loss = 0.00110694\n",
      "Iteration 55, loss = 0.00110433\n",
      "Iteration 56, loss = 0.00107959\n",
      "Iteration 57, loss = 0.00106137\n",
      "Iteration 58, loss = 0.00104594\n",
      "Iteration 59, loss = 0.00103998\n",
      "Iteration 60, loss = 0.00102648\n",
      "Iteration 61, loss = 0.00100533\n",
      "Iteration 62, loss = 0.00099340\n",
      "Iteration 63, loss = 0.00099302\n",
      "Iteration 64, loss = 0.00097092\n",
      "Iteration 65, loss = 0.00096998\n",
      "Iteration 66, loss = 0.00094576\n",
      "Iteration 67, loss = 0.00094236\n",
      "Iteration 68, loss = 0.00091827\n",
      "Iteration 69, loss = 0.00090883\n",
      "Iteration 70, loss = 0.00089879\n",
      "Iteration 71, loss = 0.00089726\n",
      "Iteration 72, loss = 0.00087148\n",
      "Iteration 73, loss = 0.00086633\n",
      "Iteration 74, loss = 0.00085206\n",
      "Iteration 75, loss = 0.00084437\n",
      "Iteration 76, loss = 0.00082731\n",
      "Iteration 77, loss = 0.00081400\n",
      "Iteration 78, loss = 0.00080753\n",
      "Iteration 79, loss = 0.00080233\n",
      "Training loss did not improve more than tol=0.000010 for 1 consecutive epochs. Stopping.\n",
      "[Voting] ...................... (5 of 5) Processing mlp, total=   2.2s\n",
      "[Voting] ..................... (1 of 5) Processing hist, total=   0.5s\n",
      "[Voting] ...................... (2 of 5) Processing ada, total=   6.2s\n",
      "[Voting] ...................... (2 of 5) Processing ada, total=   0.7s\n",
      "[Voting] ..................... (1 of 5) Processing hist, total=   0.5s\n",
      "[Voting] ...................... (2 of 5) Processing ada, total=   0.7s\n",
      "[Voting] ..................... (1 of 5) Processing hist, total=   0.4s\n",
      "Iteration 1, loss = 0.04667825\n",
      "Iteration 2, loss = 0.00779735\n",
      "Iteration 3, loss = 0.00516841\n",
      "Iteration 4, loss = 0.00415287\n",
      "Iteration 5, loss = 0.00359314\n",
      "Iteration 6, loss = 0.00324101\n",
      "Iteration 7, loss = 0.00296209\n",
      "Iteration 8, loss = 0.00274291\n",
      "Iteration 9, loss = 0.00255052\n",
      "Iteration 10, loss = 0.00241486\n",
      "Iteration 11, loss = 0.00230847\n",
      "Iteration 12, loss = 0.00222138\n",
      "Iteration 13, loss = 0.00214424\n",
      "Iteration 14, loss = 0.00208047\n",
      "Iteration 15, loss = 0.00204106\n",
      "Iteration 16, loss = 0.00198317\n",
      "Iteration 17, loss = 0.00194365\n",
      "Iteration 18, loss = 0.00190653\n",
      "Iteration 19, loss = 0.00187423\n",
      "Iteration 20, loss = 0.00183193\n",
      "Iteration 21, loss = 0.00180933\n",
      "Iteration 22, loss = 0.00178281\n",
      "Iteration 23, loss = 0.00174046\n",
      "Iteration 24, loss = 0.00171758\n",
      "Iteration 25, loss = 0.00170113\n",
      "Iteration 26, loss = 0.00168231\n",
      "Iteration 27, loss = 0.00164145\n",
      "Iteration 28, loss = 0.00162582\n",
      "Iteration 29, loss = 0.00160881\n",
      "Iteration 30, loss = 0.00157409\n",
      "Iteration 31, loss = 0.00154377\n",
      "Iteration 32, loss = 0.00152234\n",
      "Iteration 33, loss = 0.00151956\n",
      "Iteration 34, loss = 0.00149154\n",
      "Iteration 35, loss = 0.00146225\n",
      "Iteration 36, loss = 0.00143929\n",
      "Iteration 37, loss = 0.00143401\n",
      "Iteration 38, loss = 0.00140644\n",
      "Iteration 39, loss = 0.00137555\n",
      "Iteration 40, loss = 0.00135620\n",
      "Iteration 41, loss = 0.00134194\n",
      "Iteration 42, loss = 0.00131882\n",
      "Iteration 43, loss = 0.00131270\n",
      "Iteration 44, loss = 0.00128999\n",
      "Iteration 45, loss = 0.00127013\n",
      "Iteration 46, loss = 0.00126331\n",
      "Iteration 47, loss = 0.00123712\n",
      "Iteration 48, loss = 0.00123535\n",
      "Iteration 49, loss = 0.00120276\n",
      "Iteration 50, loss = 0.00119017\n",
      "Iteration 51, loss = 0.00117749\n",
      "Iteration 52, loss = 0.00115345\n",
      "Iteration 53, loss = 0.00113491\n",
      "Iteration 54, loss = 0.00112823\n",
      "Iteration 55, loss = 0.00110481\n",
      "Iteration 56, loss = 0.00109190\n",
      "Iteration 57, loss = 0.00107870\n",
      "Iteration 58, loss = 0.00106730\n",
      "Iteration 59, loss = 0.00104705\n",
      "Iteration 60, loss = 0.00102965\n",
      "Iteration 61, loss = 0.00103803\n",
      "Iteration 62, loss = 0.00099929\n",
      "Iteration 63, loss = 0.00098852\n",
      "Iteration 64, loss = 0.00096787\n",
      "Iteration 65, loss = 0.00096152\n",
      "Iteration 66, loss = 0.00094176\n",
      "Iteration 67, loss = 0.00092462\n",
      "Iteration 68, loss = 0.00093036\n",
      "Iteration 69, loss = 0.00090049\n",
      "Iteration 70, loss = 0.00088552\n",
      "Iteration 71, loss = 0.00088418\n",
      "Iteration 72, loss = 0.00087133\n",
      "Iteration 73, loss = 0.00085489\n",
      "Iteration 74, loss = 0.00085199\n",
      "Iteration 75, loss = 0.00082724\n",
      "Iteration 76, loss = 0.00082192\n",
      "Iteration 77, loss = 0.00081240\n",
      "Training loss did not improve more than tol=0.000010 for 1 consecutive epochs. Stopping.\n",
      "[Voting] ...................... (5 of 5) Processing mlp, total=   2.0s\n",
      "[Voting] ...................... (2 of 5) Processing ada, total=   0.5s\n",
      "[Voting] ...................... (3 of 5) Processing svr, total=   0.3s\n",
      "Iteration 1, loss = 0.00804339\n",
      "Iteration 2, loss = 0.00203046\n",
      "Iteration 3, loss = 0.00166372\n",
      "Iteration 4, loss = 0.00143399\n",
      "Iteration 5, loss = 0.00125890\n",
      "Iteration 6, loss = 0.00110455\n",
      "Iteration 7, loss = 0.00097491\n",
      "Iteration 8, loss = 0.00085263\n",
      "Iteration 9, loss = 0.00074926\n",
      "Iteration 10, loss = 0.00065908\n",
      "Iteration 11, loss = 0.00059278\n",
      "Iteration 12, loss = 0.00053807\n",
      "Iteration 13, loss = 0.00049048\n",
      "Iteration 14, loss = 0.00045701\n",
      "Iteration 15, loss = 0.00042777\n",
      "Iteration 16, loss = 0.00040742\n",
      "Iteration 17, loss = 0.00039188\n",
      "Iteration 18, loss = 0.00037254\n",
      "Iteration 19, loss = 0.00036082\n",
      "Iteration 20, loss = 0.00034844\n",
      "Iteration 21, loss = 0.00034497\n",
      "Iteration 22, loss = 0.00033795\n",
      "Training loss did not improve more than tol=0.000010 for 1 consecutive epochs. Stopping.\n",
      "[Voting] ...................... (5 of 5) Processing mlp, total=   6.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Voting] ...................... (3 of 5) Processing svr, total=   0.3s\n",
      "[Voting] ...................... (2 of 5) Processing ada, total=   0.5s\n",
      "Iteration 1, loss = 0.04381667\n",
      "Iteration 2, loss = 0.00722059\n",
      "Iteration 3, loss = 0.00443524\n",
      "Iteration 4, loss = 0.00358052\n",
      "Iteration 5, loss = 0.00315760\n",
      "Iteration 6, loss = 0.00284009\n",
      "Iteration 7, loss = 0.00262894\n",
      "Iteration 8, loss = 0.00245318\n",
      "Iteration 9, loss = 0.00235649\n",
      "Iteration 10, loss = 0.00225604\n",
      "Iteration 11, loss = 0.00219107\n",
      "Iteration 12, loss = 0.00213292\n",
      "Iteration 13, loss = 0.00207767\n",
      "Iteration 14, loss = 0.00201936\n",
      "Iteration 15, loss = 0.00195749\n",
      "Iteration 16, loss = 0.00192706\n",
      "Iteration 17, loss = 0.00187120\n",
      "Iteration 18, loss = 0.00184487\n",
      "Iteration 19, loss = 0.00183317\n",
      "Iteration 20, loss = 0.00178834\n",
      "Iteration 21, loss = 0.00174094\n",
      "Iteration 22, loss = 0.00171481\n",
      "Iteration 23, loss = 0.00168735\n",
      "Iteration 24, loss = 0.00165992\n",
      "Iteration 25, loss = 0.00164639\n",
      "Iteration 26, loss = 0.00161117\n",
      "Iteration 27, loss = 0.00158944\n",
      "Iteration 28, loss = 0.00156355\n",
      "Iteration 29, loss = 0.00154522\n",
      "Iteration 30, loss = 0.00152131\n",
      "Iteration 31, loss = 0.00149992\n",
      "Iteration 32, loss = 0.00148073\n",
      "Iteration 33, loss = 0.00147655\n",
      "Iteration 34, loss = 0.00144453\n",
      "Iteration 35, loss = 0.00141506\n",
      "Iteration 36, loss = 0.00142628\n",
      "Iteration 37, loss = 0.00138027\n",
      "Iteration 38, loss = 0.00136657\n",
      "Iteration 39, loss = 0.00134328\n",
      "Iteration 40, loss = 0.00133333\n",
      "Iteration 41, loss = 0.00132443\n",
      "Training loss did not improve more than tol=0.000010 for 1 consecutive epochs. Stopping.\n",
      "[Voting] ...................... (5 of 5) Processing mlp, total=   1.1s\n",
      "[Voting] ...................... (3 of 5) Processing svr, total=   0.4s\n",
      "[Voting] ...................... (3 of 5) Processing svr, total=   0.4s\n",
      "[Voting] ...................... (3 of 5) Processing svr, total=16.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    1.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leaf nodes = Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('votingregressor',\n",
      "                 VotingRegressor(estimators=[('hist',\n",
      "                                              HistGradientBoostingRegressor()),\n",
      "                                             ('ada', AdaBoostRegressor()),\n",
      "                                             ('svr', SVR()),\n",
      "                                             ('forest',\n",
      "                                              RandomForestRegressor(min_weight_fraction_leaf=0.001,\n",
      "                                                                    n_estimators=10,\n",
      "                                                                    verbose=2)),\n",
      "                                             ('mlp',\n",
      "                                              MLPRegressor(alpha=0.01,\n",
      "                                                           hidden_layer_sizes=(30,\n",
      "                                                                               30),\n",
      "                                                           learning_rate='adaptive',\n",
      "                                                           n_iter_no_change=1,\n",
      "                                                           random_state=42,\n",
      "                                                           tol=1e-05,\n",
      "                                                           verbose=1))],\n",
      "                                 n_jobs=-1, verbose=True))])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    1.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SO_H2,COCl2_CO2,CH3I_H2O,CS2_self,CH3OH_CO2,N2O_self,C2H4_H2O,HOBr_H2,PH3_CO2,SO_H2O,HO2_self,HCN_H2O,C2N2_H2O,N2O_CO2,CO_H2,HBr_self,HI_CO2,CH3OH_H2O,H2O_self,SO2_self has score = 0.05250926635368125\n",
      "mean square error = 0.01118048746785693\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "1913506\n",
      "11319326\n",
      "lets train!!!\n",
      "[Voting] ..................... (1 of 5) Processing hist, total=   2.4s\n",
      "building tree 1 of 10\n",
      "building tree 2 of 10\n",
      "building tree 3 of 10\n",
      "building tree 4 of 10\n",
      "building tree 5 of 10\n",
      "building tree 6 of 10\n",
      "building tree 7 of 10\n",
      "building tree 8 of 10\n",
      "building tree 9 of 10\n",
      "building tree 10 of 10\n",
      "[Voting] ................... (4 of 5) Processing forest, total=   2.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    2.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.00745637\n",
      "Iteration 2, loss = 0.00206766\n",
      "Iteration 3, loss = 0.00168987\n",
      "Iteration 4, loss = 0.00146128\n",
      "Iteration 5, loss = 0.00128251\n",
      "Iteration 6, loss = 0.00112704\n",
      "Iteration 7, loss = 0.00099915\n",
      "Iteration 8, loss = 0.00087685\n",
      "Iteration 9, loss = 0.00077836\n",
      "Iteration 10, loss = 0.00069572\n",
      "Iteration 11, loss = 0.00062443\n",
      "Iteration 12, loss = 0.00057266\n",
      "Iteration 13, loss = 0.00052517\n",
      "Iteration 14, loss = 0.00049002\n",
      "Iteration 15, loss = 0.00046035\n",
      "Iteration 16, loss = 0.00044061\n",
      "Iteration 17, loss = 0.00042298\n",
      "Iteration 18, loss = 0.00040583\n",
      "Iteration 19, loss = 0.00039668\n",
      "Iteration 20, loss = 0.00039012\n",
      "Training loss did not improve more than tol=0.000010 for 1 consecutive epochs. Stopping.\n",
      "[Voting] ...................... (5 of 5) Processing mlp, total=   5.9s\n",
      "[Voting] ...................... (2 of 5) Processing ada, total=   8.7s\n",
      "[Voting] ...................... (3 of 5) Processing svr, total=  16.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    1.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leaf nodes = Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('votingregressor',\n",
      "                 VotingRegressor(estimators=[('hist',\n",
      "                                              HistGradientBoostingRegressor()),\n",
      "                                             ('ada', AdaBoostRegressor()),\n",
      "                                             ('svr', SVR()),\n",
      "                                             ('forest',\n",
      "                                              RandomForestRegressor(min_weight_fraction_leaf=0.001,\n",
      "                                                                    n_estimators=10,\n",
      "                                                                    verbose=2)),\n",
      "                                             ('mlp',\n",
      "                                              MLPRegressor(alpha=0.01,\n",
      "                                                           hidden_layer_sizes=(30,\n",
      "                                                                               30),\n",
      "                                                           learning_rate='adaptive',\n",
      "                                                           n_iter_no_change=1,\n",
      "                                                           random_state=42,\n",
      "                                                           tol=1e-05,\n",
      "                                                           verbose=1))],\n",
      "                                 n_jobs=-1, verbose=True))])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COCl2_H2O,CH3F_CO2,HOCl_CO2,NO_H2O,HOCl_self,SO_CO2,CH3CN_self,SO2_H2,HCN_H2,HCl_H2,HCl_H2O,O2_H2O,CH3I_CO2,HNO3_CO2,CO_self,ClO_H2O,C2H6_CO2,COF2_H2,CH3Br_self,CH3I_H2 has score = 0.2765093047144451\n",
      "mean square error = 0.014015101588285372\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "733340\n",
      "12499492\n",
      "lets train!!!\n",
      "[Voting] ..................... (1 of 5) Processing hist, total=   2.6s\n",
      "[Voting] ...................... (2 of 5) Processing ada, total=   3.4s\n",
      "building tree 1 of 10\n",
      "building tree 2 of 10\n",
      "building tree 3 of 10\n",
      "building tree 4 of 10\n",
      "building tree 5 of 10\n",
      "building tree 6 of 10\n",
      "building tree 7 of 10\n",
      "building tree 8 of 10\n",
      "building tree 9 of 10\n",
      "building tree 10 of 10\n",
      "[Voting] ................... (4 of 5) Processing forest, total=   3.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    3.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.00782186\n",
      "Iteration 2, loss = 0.00199396\n",
      "Iteration 3, loss = 0.00166200\n",
      "Iteration 4, loss = 0.00144528\n",
      "Iteration 5, loss = 0.00124219\n",
      "Iteration 6, loss = 0.00108574\n",
      "Iteration 7, loss = 0.00095106\n",
      "Iteration 8, loss = 0.00083933\n",
      "Iteration 9, loss = 0.00074251\n",
      "Iteration 10, loss = 0.00065824\n",
      "Iteration 11, loss = 0.00059234\n",
      "Iteration 12, loss = 0.00054201\n",
      "Iteration 13, loss = 0.00049981\n",
      "Iteration 14, loss = 0.00047060\n",
      "Iteration 15, loss = 0.00044250\n",
      "Iteration 16, loss = 0.00042307\n",
      "Iteration 17, loss = 0.00040418\n",
      "Iteration 18, loss = 0.00039743\n",
      "Iteration 19, loss = 0.00038746\n",
      "Training loss did not improve more than tol=0.000010 for 1 consecutive epochs. Stopping.\n",
      "[Voting] ...................... (5 of 5) Processing mlp, total=   6.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leaf nodes = Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('votingregressor',\n",
      "                 VotingRegressor(estimators=[('hist',\n",
      "                                              HistGradientBoostingRegressor()),\n",
      "                                             ('ada', AdaBoostRegressor()),\n",
      "                                             ('svr', SVR()),\n",
      "                                             ('forest',\n",
      "                                              RandomForestRegressor(min_weight_fraction_leaf=0.001,\n",
      "                                                                    n_estimators=10,\n",
      "                                                                    verbose=2)),\n",
      "                                             ('mlp',\n",
      "                                              MLPRegressor(alpha=0.01,\n",
      "                                                           hidden_layer_sizes=(30,\n",
      "                                                                               30),\n",
      "                                                           learning_rate='adaptive',\n",
      "                                                           n_iter_no_change=1,\n",
      "                                                           random_state=42,\n",
      "                                                           tol=1e-05,\n",
      "                                                           verbose=1))],\n",
      "                                 n_jobs=-1, verbose=True))])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H2O_H2,N2O_H2,C2H2_self,C2H4_CO2,OCS_self,NO_H2,OH_self,C4H2_CO2,OCS_H2O,C2N2_H2,CH3Cl_H2O,HO2_H2,O2_self,C2H2_H2,CS_H2,H2S_self,H2O2_H2O,COF2_CO2,C4H2_H2O,HCOOH_H2 has score = -6.148608089362245\n",
      "mean square error = 0.005095082666070238\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "1648015\n",
      "11584817\n",
      "lets train!!!\n",
      "[Voting] ...................... (3 of 5) Processing svr, total=16.4min\n",
      "[Voting] ..................... (1 of 5) Processing hist, total=   2.4s\n",
      "building tree 1 of 10\n",
      "building tree 2 of 10\n",
      "building tree 3 of 10\n",
      "building tree 4 of 10\n",
      "building tree 5 of 10\n",
      "building tree 6 of 10\n",
      "building tree 7 of 10\n",
      "building tree 8 of 10\n",
      "building tree 9 of 10\n",
      "building tree 10 of 10\n",
      "[Voting] ................... (4 of 5) Processing forest, total=   3.0s\n",
      "[Voting] ...................... (2 of 5) Processing ada, total=   3.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    3.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.00733907\n",
      "Iteration 2, loss = 0.00199157\n",
      "Iteration 3, loss = 0.00164603\n",
      "Iteration 4, loss = 0.00141405\n",
      "Iteration 5, loss = 0.00122996\n",
      "Iteration 6, loss = 0.00108704\n",
      "Iteration 7, loss = 0.00096323\n",
      "Iteration 8, loss = 0.00084884\n",
      "Iteration 9, loss = 0.00075344\n",
      "Iteration 10, loss = 0.00066809\n",
      "Iteration 11, loss = 0.00059802\n",
      "Iteration 12, loss = 0.00054123\n",
      "Iteration 13, loss = 0.00049872\n",
      "Iteration 14, loss = 0.00046327\n",
      "Iteration 15, loss = 0.00044017\n",
      "Iteration 16, loss = 0.00041188\n",
      "Iteration 17, loss = 0.00039530\n",
      "Iteration 18, loss = 0.00038046\n",
      "Iteration 19, loss = 0.00036947\n",
      "Iteration 20, loss = 0.00036048\n",
      "Iteration 21, loss = 0.00035310\n",
      "Training loss did not improve more than tol=0.000010 for 1 consecutive epochs. Stopping.\n",
      "[Voting] ...................... (5 of 5) Processing mlp, total=   6.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leaf nodes = Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('votingregressor',\n",
      "                 VotingRegressor(estimators=[('hist',\n",
      "                                              HistGradientBoostingRegressor()),\n",
      "                                             ('ada', AdaBoostRegressor()),\n",
      "                                             ('svr', SVR()),\n",
      "                                             ('forest',\n",
      "                                              RandomForestRegressor(min_weight_fraction_leaf=0.001,\n",
      "                                                                    n_estimators=10,\n",
      "                                                                    verbose=2)),\n",
      "                                             ('mlp',\n",
      "                                              MLPRegressor(alpha=0.01,\n",
      "                                                           hidden_layer_sizes=(30,\n",
      "                                                                               30),\n",
      "                                                           learning_rate='adaptive',\n",
      "                                                           n_iter_no_change=1,\n",
      "                                                           random_state=42,\n",
      "                                                           tol=1e-05,\n",
      "                                                           verbose=1))],\n",
      "                                 n_jobs=-1, verbose=True))])\n",
      "[Voting] ...................... (3 of 5) Processing svr, total=14.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NH3_H2O,OH_H2O,C2H2_H2O,HF_CO2,CH3OH_self,CO2_CO2,HF_self,CH3Br_H2O,C2H6_H2O,HOCl_H2O,H2O2_H2,H2S_H2O,GeH4_self,CO2_H2,HI_self,ClO_CO2,COCl2_self,C2H6_self,C2H2_CO2,CO2_H2O has score = 0.25525074559830063\n",
      "mean square error = 0.004154588260896388\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "1508498\n",
      "11724334\n",
      "lets train!!!\n",
      "[Voting] ..................... (1 of 5) Processing hist, total=   2.4s\n",
      "building tree 1 of 10\n",
      "building tree 2 of 10\n",
      "building tree 3 of 10\n",
      "building tree 4 of 10\n",
      "building tree 5 of 10\n",
      "building tree 6 of 10\n",
      "building tree 7 of 10\n",
      "building tree 8 of 10\n",
      "building tree 9 of 10\n",
      "building tree 10 of 10\n",
      "[Voting] ................... (4 of 5) Processing forest, total=   3.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    3.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Voting] ...................... (2 of 5) Processing ada, total=   4.8s\n",
      "Iteration 1, loss = 0.00720703\n",
      "Iteration 2, loss = 0.00205356\n",
      "Iteration 3, loss = 0.00167835\n",
      "Iteration 4, loss = 0.00144158\n",
      "Iteration 5, loss = 0.00126359\n",
      "Iteration 6, loss = 0.00111726\n",
      "Iteration 7, loss = 0.00098199\n",
      "Iteration 8, loss = 0.00087506\n",
      "Iteration 9, loss = 0.00077677\n",
      "Iteration 10, loss = 0.00068633\n",
      "Iteration 11, loss = 0.00061738\n",
      "Iteration 12, loss = 0.00056749\n",
      "Iteration 13, loss = 0.00051808\n",
      "Iteration 14, loss = 0.00048385\n",
      "Iteration 15, loss = 0.00045642\n",
      "Iteration 16, loss = 0.00043393\n",
      "Iteration 17, loss = 0.00042166\n",
      "Iteration 18, loss = 0.00040191\n",
      "Iteration 19, loss = 0.00038855\n",
      "Iteration 20, loss = 0.00037749\n",
      "Iteration 21, loss = 0.00036965\n",
      "Iteration 22, loss = 0.00036044\n",
      "Training loss did not improve more than tol=0.000010 for 1 consecutive epochs. Stopping.\n",
      "[Voting] ...................... (5 of 5) Processing mlp, total=   6.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Voting] ...................... (3 of 5) Processing svr, total=  16.5s\n",
      "leaf nodes = Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('votingregressor',\n",
      "                 VotingRegressor(estimators=[('hist',\n",
      "                                              HistGradientBoostingRegressor()),\n",
      "                                             ('ada', AdaBoostRegressor()),\n",
      "                                             ('svr', SVR()),\n",
      "                                             ('forest',\n",
      "                                              RandomForestRegressor(min_weight_fraction_leaf=0.001,\n",
      "                                                                    n_estimators=10,\n",
      "                                                                    verbose=2)),\n",
      "                                             ('mlp',\n",
      "                                              MLPRegressor(alpha=0.01,\n",
      "                                                           hidden_layer_sizes=(30,\n",
      "                                                                               30),\n",
      "                                                           learning_rate='adaptive',\n",
      "                                                           n_iter_no_change=1,\n",
      "                                                           random_state=42,\n",
      "                                                           tol=1e-05,\n",
      "                                                           verbose=1))],\n",
      "                                 n_jobs=-1, verbose=True))])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CH3Br_H2,C2N2_self,CO2_self,CH3F_H2O,H2_H2,H2CO_self,CH3F_self,C2H4_self,PH3_self,OH_CO2,NO2_self,H2O2_self,HC3N_CO2,OCS_H2,C2H6_H2,COF2_H2O,H2CO_H2O,H2CO_CO2,ClO_H2,HC3N_self has score = 0.762019981635773\n",
      "mean square error = 0.006644792386343759\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "1213605\n",
      "12019227\n",
      "lets train!!!\n",
      "[Voting] ..................... (1 of 5) Processing hist, total=   2.5s\n",
      "building tree 1 of 10\n",
      "building tree 2 of 10\n",
      "building tree 3 of 10\n",
      "building tree 4 of 10\n",
      "building tree 5 of 10\n",
      "building tree 6 of 10\n",
      "building tree 7 of 10\n",
      "building tree 8 of 10\n",
      "building tree 9 of 10\n",
      "building tree 10 of 10\n",
      "[Voting] ................... (4 of 5) Processing forest, total=   3.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    3.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Voting] ...................... (2 of 5) Processing ada, total=   4.2s\n",
      "Iteration 1, loss = 0.00780819\n",
      "Iteration 2, loss = 0.00211886\n",
      "Iteration 3, loss = 0.00176885\n",
      "Iteration 4, loss = 0.00154204\n",
      "Iteration 5, loss = 0.00133818\n",
      "Iteration 6, loss = 0.00116561\n",
      "Iteration 7, loss = 0.00102824\n",
      "Iteration 8, loss = 0.00090443\n",
      "Iteration 9, loss = 0.00080400\n",
      "Iteration 10, loss = 0.00072061\n",
      "Iteration 11, loss = 0.00064284\n",
      "Iteration 12, loss = 0.00058656\n",
      "Iteration 13, loss = 0.00053452\n",
      "Iteration 14, loss = 0.00050168\n",
      "Iteration 15, loss = 0.00047321\n",
      "Iteration 16, loss = 0.00045214\n",
      "Iteration 17, loss = 0.00042997\n",
      "Iteration 18, loss = 0.00041396\n",
      "Iteration 19, loss = 0.00040199\n",
      "Iteration 20, loss = 0.00038670\n",
      "Iteration 21, loss = 0.00037947\n",
      "Iteration 22, loss = 0.00037241\n",
      "Training loss did not improve more than tol=0.000010 for 1 consecutive epochs. Stopping.\n",
      "[Voting] ...................... (5 of 5) Processing mlp, total=   6.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leaf nodes = Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('votingregressor',\n",
      "                 VotingRegressor(estimators=[('hist',\n",
      "                                              HistGradientBoostingRegressor()),\n",
      "                                             ('ada', AdaBoostRegressor()),\n",
      "                                             ('svr', SVR()),\n",
      "                                             ('forest',\n",
      "                                              RandomForestRegressor(min_weight_fraction_leaf=0.001,\n",
      "                                                                    n_estimators=10,\n",
      "                                                                    verbose=2)),\n",
      "                                             ('mlp',\n",
      "                                              MLPRegressor(alpha=0.01,\n",
      "                                                           hidden_layer_sizes=(30,\n",
      "                                                                               30),\n",
      "                                                           learning_rate='adaptive',\n",
      "                                                           n_iter_no_change=1,\n",
      "                                                           random_state=42,\n",
      "                                                           tol=1e-05,\n",
      "                                                           verbose=1))],\n",
      "                                 n_jobs=-1, verbose=True))])\n",
      "[Voting] ...................... (3 of 5) Processing svr, total=15.9min\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Machine learning!\n",
    "#print('Can you get to the start of ML?')\n",
    "\n",
    "## start machine learning\n",
    "#total_1oerr2 = 0\n",
    "#total_1oerr = 0\n",
    "#total_err = 0\n",
    "#total_err2 = 0\n",
    "#total_1orterr = 0\n",
    "#total_nowt = 0\n",
    "#total_MLP = 0\n",
    "counter = 0\n",
    "pipe_container = []\n",
    "\n",
    "total = 0\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import uniform, lognorm\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "#>>> from sklearn.pipeline import Pipeline\n",
    "#>>> import numpy as np\n",
    "flag = False\n",
    "#kernel = 1.0 * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e3))\n",
    "# learn on each molecule\n",
    "for key, data in molecule_list.items():\n",
    "    print(len(data[1]))\n",
    "    #print(key)\n",
    "    #molecule, broadener = key.split('_')\n",
    "    # print out which molecule we're looking at\n",
    "    #counter += 1\n",
    "    #if counter < 1:\n",
    "    #    continue\n",
    "    \n",
    "    #distributions = dict(histgradientboostingregressor__loss=['least_squares', 'least_absolute_deviation', 'poisson'],# 'quantile'],\n",
    "    #                      histgradientboostingregressor__learning_rate=lognorm(2),         histgradientboostingregressor__learning_rate=uniform(loc=0.2, scale=0.9))\n",
    "    #                      #histgradientboostingregressor__max_iter=np.random.randint(10, 1000, 1000),       #lognorm(1.5, scale=300),\n",
    "    #                      #histgradientboostingregressor__max_leaf_nodes=np.random.randint(2, 100, 1000),              #uniform(loc=0, scale=300),\n",
    "    #                      #histgradientboostingregressor__l2_regularization=uniform(loc=0, scale=10)) \n",
    "    \n",
    "\n",
    "\n",
    "    #distributions = dict(histgradientboostingregressor__min_samples_leaf=[1000, 100, 20, 5, 1])\n",
    "    #distributions = dict(mlpregressor__hidden_layer_sizes=[(100, 100), (1000, 1000), (5000, 500), (500, 5000)], \n",
    "    #                     mlpregressor__activation=['tanh', 'relu'],\n",
    "    #                     mlpregressor__alpha=[0.0001, 0.001, 0.01])\n",
    "    \n",
    "    #if key == 'HO2':\n",
    "    #    flag=True\n",
    "    #elif key == 'NO':\n",
    "    #    flag=False\n",
    "    #print(molecule)\n",
    "    #if flag == False:\n",
    "    #    continue\n",
    "    \n",
    "    \n",
    "    # take out training and test data\n",
    "    data_train = data[0]\n",
    "    data_test = data[1]\n",
    "    \n",
    "    # Check if there are null values\n",
    "    #if data_train.isnull().values.any():\n",
    "    #    raise ValueError(\"We're getting null values here, might be good to cut them out\")\n",
    "\n",
    "    # shuffle data, randomised lines of each molecule for machine learning\n",
    "    print(len(data_train))\n",
    "    data_test = data_test.sample(frac=1)\n",
    "    print(\"lets train!!!\")\n",
    "    data_train = data_train.sample(frac=0.01)\n",
    "    \n",
    "\n",
    "    #print('data splitted')\n",
    "\n",
    "\n",
    "\n",
    "    # Training data - separate out all x values and y (broadening) values.  gamma-err is weighting\n",
    "    X_train = data_train.drop(['gamma', 'gamma-err'], axis=1)\n",
    "    y_train = data_train['gamma']\n",
    "    weight_train = data_train['gamma-err']\n",
    "    #weight_train_1ovsquare = data_train['gamma_air-err']\n",
    "    #weight_train_1overr = np.sqrt(data_train['gamma_air-err'])\n",
    "    #weight_train_err = 1/np.sqrt(data_train['gamma_air-err'])\n",
    "    #weight_train_err_sq = 1/data_train['gamma_air-err']\n",
    "    #weight_train_1ovsq_root = np.sqrt(np.sqrt(data_train['gamma_air-err']))\n",
    "    #weight_train = np.sqrt(data_train['gamma_air-err'])\n",
    "    #weight_train = np.sqrt(data_train['gamma_air-err'])\n",
    "    # Separate out test data\n",
    "    X_test = data_test.drop(['gamma', 'gamma-err'], axis=1)\n",
    "    y_test = data_test['gamma']\n",
    "    weight_test = data_test['gamma-err']\n",
    "    #weight_test_1ovsquare = data_test['gamma_air-err']\n",
    "    #weight_test_1overr = np.sqrt(data_test['gamma_air-err'])\n",
    "    #weight_test_err = 1/np.sqrt(data_test['gamma_air-err'])\n",
    "    #weight_test_err_sq = 1/data_test['gamma_air-err']\n",
    "    #weight_test_1ovsq_root = np.sqrt(np.sqrt(data_test['gamma_air-err']))\n",
    "\n",
    "    \n",
    "    #pipe = make_pipeline(StandardScaler(), RandomForestRegressor(n_estimators=10, min_weight_fraction_leaf=0.001, verbose=2))\n",
    "\n",
    "    #FINALLLLL\n",
    "    \n",
    "    # Create pipeline of scaling, then ML method\n",
    "    pipe = make_pipeline(StandardScaler(), VotingRegressor(\n",
    "                                               estimators=[('hist', HistGradientBoostingRegressor()),\n",
    "                                                           ('ada', AdaBoostRegressor()),\n",
    "                                                           ('svr', SVR()),\n",
    "                                                           ('forest', RandomForestRegressor(n_estimators=10, min_weight_fraction_leaf=0.001, verbose=2)),\n",
    "                                                           ('mlp', MLPRegressor(hidden_layer_sizes=(30, 30), alpha=0.01, learning_rate='adaptive', random_state=42, verbose=1, n_iter_no_change=1, tol=0.00001))]\n",
    "                                                , n_jobs=-1, verbose=True\n",
    "                                               ))\n",
    "\n",
    "    \n",
    "    '''\n",
    "    pipe1 = make_pipeline(StandardScaler(), VotingRegressor(\n",
    "                                               estimators=[#('hist', HistGradientBoostingRegressor(learning_rate=1, max_leaf_nodes=None, random_state=42)),\n",
    "                                                           ('ada', AdaBoostRegressor()),\n",
    "                                                           ('svr', SVR()),\n",
    "                                                           ('forest', RandomForestRegressor(n_estimators=10, min_weight_fraction_leaf=0.001, verbose=2)),\n",
    "                                                           ('mlp', MLPRegressor(hidden_layer_sizes=(30, 30), alpha=0.01, learning_rate='adaptive', random_state=42, verbose=1, n_iter_no_change=1, tol=0.00001))]\n",
    "                                                , n_jobs=-1, verbose=True\n",
    "                                               ))\n",
    "    pipe2 = make_pipeline(StandardScaler(), VotingRegressor(\n",
    "                                               estimators=[('hist', HistGradientBoostingRegressor(learning_rate=1, max_leaf_nodes=None, random_state=42)),\n",
    "                                                           #('ada', AdaBoostRegressor()),\n",
    "                                                           ('svr', SVR()),\n",
    "                                                           ('forest', RandomForestRegressor(n_estimators=10, min_weight_fraction_leaf=0.001, verbose=2)),\n",
    "                                                           ('mlp', MLPRegressor(hidden_layer_sizes=(30, 30), alpha=0.01, learning_rate='adaptive', random_state=42, verbose=1, n_iter_no_change=1, tol=0.00001))]\n",
    "                                                , n_jobs=-1, verbose=True\n",
    "                                               ))\n",
    "    pipe3 = make_pipeline(StandardScaler(), VotingRegressor(\n",
    "                                               estimators=[('hist', HistGradientBoostingRegressor(learning_rate=1, max_leaf_nodes=None, random_state=42)),\n",
    "                                                           ('ada', AdaBoostRegressor()),\n",
    "                                                           #('svr', SVR()),\n",
    "                                                           ('forest', RandomForestRegressor(n_estimators=10, min_weight_fraction_leaf=0.001, verbose=2)),\n",
    "                                                           ('mlp', MLPRegressor(hidden_layer_sizes=(30, 30), alpha=0.01, learning_rate='adaptive', random_state=42, verbose=1, n_iter_no_change=1, tol=0.00001))]\n",
    "                                                , n_jobs=-1, verbose=True\n",
    "                                               ))\n",
    "    pipe4 = make_pipeline(StandardScaler(), VotingRegressor(\n",
    "                                               estimators=[('hist', HistGradientBoostingRegressor(learning_rate=1, max_leaf_nodes=None, random_state=42)),\n",
    "                                                           ('ada', AdaBoostRegressor()),\n",
    "                                                           ('svr', SVR()),\n",
    "                                                           #('forest', RandomForestRegressor(n_estimators=10, min_weight_fraction_leaf=0.001, verbose=2)),\n",
    "                                                           ('mlp', MLPRegressor(hidden_layer_sizes=(30, 30), alpha=0.01, learning_rate='adaptive', random_state=42, verbose=1, n_iter_no_change=1, tol=0.00001))]\n",
    "                                                , n_jobs=-1, verbose=True\n",
    "                                               ))\n",
    "    pipe5 = make_pipeline(StandardScaler(), VotingRegressor(\n",
    "                                               estimators=[('hist', HistGradientBoostingRegressor(learning_rate=1, max_leaf_nodes=None, random_state=42)),\n",
    "                                                           ('ada', AdaBoostRegressor()),\n",
    "                                                           ('svr', SVR()),\n",
    "                                                           ('forest', RandomForestRegressor(n_estimators=10, min_weight_fraction_leaf=0.001, verbose=2))]\n",
    "                                                           #('mlp', MLPRegressor(hidden_layer_sizes=(30, 30), alpha=0.01, learning_rate='adaptive', random_state=42, verbose=1, n_iter_no_change=1, tol=0.00001))]\n",
    "                                                , n_jobs=-1, verbose=True\n",
    "                                               ))\n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "\n",
    "    #('mlp', MLPRegressor(hidden_layer_sizes=(30, 30), alpha=0.01, learning_rate='adaptive', random_state=42, verbose=1, n_iter_no_change=1, tol=0.00001))]\n",
    "\n",
    "    #pipe = make_pipeline(StandardScaler(), HistGradientBoostingRegressor(learning_rate=1, max_leaf_nodes=None, random_state=42, verbose=1))\n",
    "    #pipe = make_pipeline(StandardScaler(), HistGradientBoostingRegressor(random_state=42, verbose=1))\n",
    "    #pipe = RandomizedSearchCV(pipe, distributions, n_iter=5, cv=2, verbose=3)\n",
    "    #pipe = make_pipeline(StandardScaler(), AdaBoostRegressor())\n",
    "    #pipe = make_pipeline(StandardScaler(),  SGDRegressor(max_iter=1000, tol=1e-3))\n",
    "    #pipe = make_pipeline(StandardScaler(), DummyRegressor(strategy='mean'))\n",
    "    #pipe = make_pipeline(StandardScaler(), DecisionTreeRegressor(max_depth=9))\n",
    "    #pipe = make_pipeline(StandardScaler(), RandomForestRegressor())\n",
    "    #pipe = make_pipeline(StandardScaler(), MLPRegressor(hidden_layer_sizes=(100, 100), alpha=0.001, learning_rate='adaptive', random_state=42, verbose=1, n_iter_no_change=3, tol=0.00001))#, hidden_layer_sizes=(10, 100), alpha=0.001, learning_rate='adaptive', random_state=42, verbose=1))\n",
    "    #pipe = make_pipeline(StandardScaler(), PolynomialFeatures(degree=3), LinearRegression(fit_intercept=False))\n",
    "    #pipe = make_pipeline(StandardScaler(), SVR(C=5, epsilon=0.2))\n",
    "    #pipe = make_pipeline(StandardScaler(), GaussianProcessRegressor(kernel=kernel, alpha=0.1))\n",
    "    #pipe = RandomizedSearchCV(pipe, distributions, n_iter=24, cv=2, verbose=3)\n",
    "\n",
    "    #print('Pipeline made')\n",
    "    #print(pipe) \n",
    "    #pipe.fit(X_train, y_train, linearregression__sample_weight=weight_train)\n",
    "    #pipe.fit(X_train, y_train, histgradientboostingregressor__sample_weight=weight_train)\n",
    "    #pipe.fit(X_train, y_train, sgdregressor__sample_weight=weight_train)\n",
    "    #pipe.fit(X_train, y_train, dummyregressor__sample_weight=weight_train)\n",
    "    #pipe.fit(X_train, y_train, adaboostregressor__sample_weight=weight_train)\n",
    "    #pipe.fit(X_train, y_train, decisiontreeregressor__sample_weight=weight_train)\n",
    "    #pipe.fit(X_train, y_train, randomforestregressor__sample_weight=weight_train)\n",
    "    #pipe.fit(X_train, y_train, votingregressor__sample_weight=weight_train)\n",
    "    pipe.fit(X_train, y_train)#, gaussianprocessregressor__sample_weight=weight_train)\n",
    "    #pipe.fit(X_train, y_train, mlpregressor__sample_weight=weight_train)\n",
    "    '''pipe1.fit(X_train, y_train)#, mlpregressor__sample_weight=weight_train)\n",
    "    pipe2.fit(X_train, y_train)#, mlpregressor__sample_weight=weight_train)\n",
    "    pipe3.fit(X_train, y_train)#, mlpregressor__sample_weight=weight_train)\n",
    "    pipe4.fit(X_train, y_train)#, mlpregressor__sample_weight=weight_train)\n",
    "    pipe5.fit(X_train, y_train)#, mlpregressor__sample_weight=weight_train)\n",
    "    '''\n",
    "\n",
    "    # Predict broadening values\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    '''y_pred1 = pipe.predict(X_test)\n",
    "    y_pred2 = pipe.predict(X_test)\n",
    "    y_pred3 = pipe.predict(X_test)\n",
    "    y_pred4 = pipe.predict(X_test)\n",
    "    y_pred5 = pipe.predict(X_test)\n",
    "    '''\n",
    "\n",
    "    #print(pipe.best_params_)\n",
    "    #y_pred2 = pipe2.predict(X_test)\n",
    "    #y_pred3 = pipe3.predict(X_test)\n",
    "    #y_pred4 = pipe4.predict(X_test)\n",
    "    #y_pred5 = pipe5.predict(X_test)\n",
    "    #y_pred6 = pipe6.predict(X_test)\n",
    "    #y_pred7 = pipe7.predict(X_test)\n",
    "\n",
    "\n",
    "    print('leaf nodes = '+str(pipe))\n",
    "\n",
    "    #print('fitting done')\n",
    "\n",
    "\n",
    "\n",
    "    # print out the scor\n",
    "    score = pipe.score(X_test, y_test, sample_weight=weight_test)\n",
    "    mse_score = mean_squared_error(y_pred, y_test, sample_weight=weight_test)\n",
    "    \n",
    "    '''score1 = pipe1.score(X_test, y_test, sample_weight=weight_test)\n",
    "    score2 = pipe2.score(X_test, y_test, sample_weight=weight_test)\n",
    "    score3 = pipe3.score(X_test, y_test, sample_weight=weight_test)\n",
    "    score4 = pipe4.score(X_test, y_test, sample_weight=weight_test)\n",
    "    score5 = pipe5.score(X_test, y_test, sample_weight=weight_test)\n",
    "    '''\n",
    "    #score2 = pipe2.score(X_test, y_test, weight_test_1ovsquare)\n",
    "    #score3 = pipe3.score(X_test, y_test, weight_test_1ovsquare)\n",
    "    #score4 = pipe4.score(X_test, y_test, weight_test_1ovsquare)\n",
    "    #score5 = pipe5.score(X_test, y_test, weight_test_1ovsquare)\n",
    "    #score6 = pipe6.score(X_test, y_test, weight_test_1ovsquare)\n",
    "    #score7 = pipe7.score(X_test, y_test, weight_test_1ovsquare)\n",
    "    \n",
    "    print(key+' has score = '+str(score))\n",
    "    print('mean square error = '+str(mse_score))\n",
    "\n",
    "    print()\n",
    "    print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    '''print('minor scores = ')\n",
    "    print(score1, score2, score3, score4, score5)\n",
    "    '''\n",
    "\n",
    "    #print('score 1/err = '+str(score2))\n",
    "    #print('score err = '+str(score3))\n",
    "    #print('score err2 = '+str(score4))\n",
    "    #print('score 1/rt(err) = '+str(score5))\n",
    "    #print('score nowt = '+str(score6))\n",
    "    #print('score MLP = '+str(score7))\n",
    "    \n",
    "    # Add up the total score - allows methods to be compared\n",
    "    if score > -10:\n",
    "        total += score\n",
    "    #total_1oerr2 += score1\n",
    "    #total_1oerr += score2\n",
    "    #total_err += score3\n",
    "    #total_err2 += score4\n",
    "    #total_1orterr += score5\n",
    "    #total_nowt += score6\n",
    "    #total_MLP = score7\n",
    "    \n",
    "    \n",
    "    # Get data into matplotlib friendly form\n",
    "    y_test_plot = y_test.to_numpy()\n",
    "    x_plot = X_test['M'].to_numpy()\n",
    "\n",
    "\n",
    "    # prepare to plot different accuracy different colour\n",
    "    #err_codes = data_test['gamma_air-err'].value_counts().sort_index()\n",
    "    #data_by_vib_lev = {}\n",
    "    \n",
    "    '''\n",
    "    cv_results = pd.DataFrame(pipe.cv_results_).sort_values(\"mean_test_score\", ascending=False)\n",
    "    column_results = [f\"param_{name}\" for name in distributions.keys()]\n",
    "    column_results += [\"mean_test_score\", \"std_test_score\", \"rank_test_score\"]\n",
    "    cv_results = cv_results[column_results]\n",
    "    def shorten_param(param_name):\n",
    "        if \"__\" in param_name:\n",
    "            return param_name.rsplit(\"__\", 1)[1]\n",
    "        return param_name\n",
    "    cv_results = cv_results.rename(shorten_param, axis=1)\n",
    "    pivoted_cv_results = cv_results.pivot_table(values=\"mean_test_score\", index=[\"hidden_layer_sizes\"], columns=[\"alpha\"])\n",
    "    print('CV results:')\n",
    "    print(cv_results)\n",
    "    print('LR vs l2')\n",
    "    print(pivoted_cv_results)\n",
    "    '''\n",
    "   \n",
    "    #plot_data_list.append([key, x_plot[-100000:], y_test_plot[-100000:], y_pred[-100000:], [score, score1, score2, score3, score4, score5], mse_score, X_test['molecule_weight'][-100000:]])#, dot_data])#, pipe])\n",
    "    plot_data_list.append([key, x_plot[-100000:], y_test_plot[-100000:], y_pred[-100000:], score, mse_score, X_test['molecule_weight'][-100000:]])#, dot_data])#, pipe])\n",
    "    \n",
    "\n",
    "    \n",
    "    #print(pipe[1].train_score_)\n",
    "    \n",
    "    #pipe_container.append([pipe, pipe1, pipe2, pipe3, pipe4, pipe5])\n",
    "    pipe_container.append(pipe)\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "\n",
    "\n",
    "print('total score = '+str(total))\n",
    "\n",
    "import pickle \n",
    "\n",
    "with open('final_10-25_voter_data.pkl', 'wb') as f:\n",
    "    pickle.dump(plot_data_list, f)\n",
    "\n",
    "with open('final_10-25_voter_forest.pkl', 'wb') as f:\n",
    "    pickle.dump(pipe_container, f)\n",
    "\n",
    "\n",
    "#with open(\"baseline.csv\", \"w\") as f:\n",
    "#    wr = csv.writer(f)\n",
    "#    wr.writerows(plot_data_list)\n",
    "'''\n",
    "with open('baseline_results.json', 'wb') as fp:\n",
    "    json.dump(plot_data_list, fp)'''\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-03-13T08:26:41.411260Z"
    }
   },
   "id": "ca7446a9903dd541"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "918512aaa25cda8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5577267d1852e186"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
